---
emoji: ğŸ§¢
title: TreeGen
date: '2023-08-02 22:00:00'
author: DolmaengC
tags: CodeGeneration DL Transformer
categories: CodeGeneration 
---



# TreeGen: A Tree-Based Transformer Architecture for Code Generation

**Zeyu Sunâ€  **

**Qihao Zhuâ€  **

**Yingfei Xiongâˆ—â€  **

**Yican Sunâ€  Lili Mouâ€¡ **

**Lu Zhangâ€ **



## ABSTRCT

ì½”ë“œ ìƒì„± ì‹œìŠ¤í…œì€ ìì—°ì–´ ë¬˜ì‚¬ë¥¼ ì¸í’‹ìœ¼ë¡œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ì½”ë“œë¥¼ ìƒì„±í•œë‹¤.

ìµœì‹  ê¸°ìˆ ë“¤ì€ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ì— ì˜ì¡´í•˜ëŠ”ë° , 2ê°€ì§€ ë¬¸ì œì ì´ ìˆë‹¤.

1. ê¸´ ì˜ì¡´ì„± ë¬¸ì œ
   - ì½”ë“œëŠ” ì¢…ì¢… ë©€ë¦¬ìˆëŠ” ì½”ë“œ ìš”ì†Œì˜ ì˜í–¥ì„ ë°›ëŠ”ë‹¤.
2. ëª¨ë¸ë§ êµ¬ì¡°
   - ë§ì€ êµ¬ì¡°ì  ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‹¤.



TreeGen : ìƒˆë¡œìš´ íŠ¸ë¦¬ ê¸°ë°˜ ë‰´ëŸ´ êµ¬ì¡° (íŠ¸ë ŒìŠ¤í¬ë¨¸ì˜ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©)



ë²¤ì¹˜ë§ˆí¬

- íŒŒì´ì¬ ë²¤ì¹˜ë§ˆí¬ : HearthStone
- semantic parsing ë²¤ì¹˜ë§ˆí¬ : ATIS, GEO



# INTRODUCTION

ì½”ë“œ ìƒì„±ì€ ê°œë°œìë“¤ì˜ ìƒì‚°ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì¤‘ìš”í•œ ì¸ê³µì§€ëŠ¥ì´ë‹¤.

Seq2Seq, Seq2Tree ëª¨ë¸ ë“± ë‹¤ì–‘í•œ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ê°–ëŠ”ë‹¤.

ìµœì‹  ì ‘ê·¼ë²•ì€ ë¬¸ë²• ê·œì¹™ì˜ sequenceë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì½”ë“œë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ë‹¤.



ì´ ë…¼ë¬¸ì€ ìƒˆë¡œìš´ ì‹ ê²½ë§ êµ¬ì¡° (TreeGen)ì„ ì œì•ˆí•œë‹¤.

TreeGenì€ íŠ¸ë ŒìŠ¤í¬ë¨¸ êµ¬ì¡°ë¥¼ ì œì•ˆí•˜ì§€ë§Œ, ê¸°ì¡´ì˜ íŠ¸ë ŒìŠ¤í¬ë¨¸ëŠ” í”„ë¡œê·¸ë¨ìš©ìœ¼ë¡œ ì„¤ê³„ë˜ì–´ ìˆì§€ ì•Šì•˜ìœ¼ë©°,

íŠ¸ë¦¬ êµ¬ì¡°ì— ìµœì í™” ë˜ì–´ ìˆì§€ ì•Šë‹¤.


ìµœì í™”ë¥¼ ìœ„í•´ì„œëŠ” ê·¸ë˜í”„, íŠ¸ë¦¬ ê¸°ë°˜ì˜ ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ê°€ì ¸ì•¼ í•œë‹¤.



TreeGenì€ 3ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±í•œë‹¤.

1. A natural language reader (encoder) : encode the text description.

2. A AST reader (the first several transformer decoder blocks) : 

   encode the previously generated partial code with the structural convolution sub-layers

3. A decoder (the rest transformer decoder blocks) : 

   combine the query and previous two encoders to predict the next grammer rule.





## TreeGen

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-01 á„‹á…©á„’á…® 2.22.20](/Users/choejunhyeog/Desktop/á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-01 á„‹á…©á„’á…® 2.22.20.png)

### 1. Grammar Rule Prediction

decomposed into several context-free grammar rules and parsed as an AST

AST-based code generation could be thought of as expanding a non-terminal node by a grammar rule. This process is repeated until all leaf nodes are terminal.

ì„ ì£¼ë¬¸ íŠ¸ë˜ë²„ìŠ¤ì— ë”°ë¼ ì˜¤ë¥¸ìª½ ìƒë‹¨ ëª¨ì„œë¦¬ì— í‘œì‹œëœ ASTë¥¼ ìƒì„±í•˜ëŠ” ì¼ë ¨ì˜ ê·œì¹™ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Formally, the probability can be factorized as the proba- bilities of the rules generating the code following the order.

![1.png](1.png)

our task is to train a model to calculate p(ri | NL input, pi )



### 2. NL Reader

The input description determines the functionality of the code.

1. Tokenize input description into tokens.
2. All the tokens and characters are represented as real-valued vectors n1, n2,...,nL and c1, c2, ... , cs by embeddings. 



In summary, the NL reader has a few Transformer blocks

of self-attention, the gating mechanism, and word convolu-

tion. The natural language description is encoded as features y(NL), y(NL), Â· Â· Â· , y(NL).



### 3. Input Text Representation

#### Character Embedding

Similar words have similar characters (e.g. "program" and "programs")

a token by character embeddings with **a fully-connected layer**

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-01 á„‹á…©á„’á…® 5.37.43](/Users/choejunhyeog/Desktop/á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-01 á„‹á…©á„’á…® 5.37.43.png)

W(c) are the weights and the character sequence is padded to a pre-defined maximum length M.

**Layer normalization**

ì´ ë²¡í„°ë“¤ì€ NL readerë¡œ ë„˜ì–´ê°€ê³  **gating sub-layer**ì—ì„œ ì›Œë“œ ì„ë² ë”©ê³¼ í†µí•©ëœë‹¤.



### 4. Neural Structure of NL Reader

The NL reader is composed of a stack of blocks.

Each block contains three different sub-layers (self-attention, gating mechanism, word convolution)



#### Self-Attention

Transformer's architecture

Multi-head attention (to capture long dependency information)



1. Embedding by a look-up table
2. position embeddings
3. Variant (Dehghani et al.)[https://arxiv.org/abs/1807.03819]
4. Compute the position embedding for the 4th word in the word in the bth Transformer block as

![34.png](34.png)



#### Gating Mechanism

Self-attentionì— ì˜í•´ íŠ¹ì§•ë“¤ì´ ê³„ì‚°ëœ í›„ì—, character ì„ë² ë”© ì •ë³´ì™€ í†µí•©í•œë‹¤.

gating mechanismì€ softmaxë¥¼ ë² ì´ìŠ¤ë¡œ í•œë‹¤.



#### Word Convolution

2ê°œì˜ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ê°€ gating ë©”ì¹´ë‹ˆì¦˜ì˜ ì•„ì›ƒí’‹ì— ì ìš©ëœë‹¤.

ì—¬ê¸°ì„œ seperable convolutionì´ ì‚¬ìš©ëœë‹¤.

-  seperable convolutionì€ íŒŒë¼ë¯¸í„°ê°€ ì ì–´ì„œ í•™ìŠµì´ ì‰½ë‹¤.

ì²«ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ë‹¨ì–´ë¥¼ ìœ„í•´ zero paddingì„ ì¶”ê°€í•œë‹¤.

GELU í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©



### 5. AST Reader

ìƒì„±ëœ ë¶€ë¶„ì  ASTì˜ êµ¬ì¡°ë¥¼ ëª¨ë¸ë§í•˜ê¸° ìœ„í•´ AST readerë¥¼ ë§Œë“¤ì—ˆë‹¤.

ë¬¸ë²• ê·œì¹™ì˜ ìˆœì„œë¥¼ ì˜ˆì¸¡í•˜ì—¬ ìƒì„±í•˜ì§€ë§Œ, ì´ ê·œì¹™ë“¤ë§Œìœ¼ë¡œëŠ” í”„ë¡œê·¸ë¨ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ê·¸ë¦¼ì´ ë¶€ì¡±í•˜ê³  ë‹¤ìŒ ê·œì¹™ì„ ì˜ˆì¸¡í•˜ê¸°ì—ëŠ” ë¶ˆì¶©ë¶„í•˜ë‹¤.

-> AST readerê°€ ì˜ˆì¸¡ëœ ì •ë³´ì™€ íŠ¸ë¦¬êµ¬ì¡°ë¥¼ í¬í•¨í•œ heterogeneous ì •ë³´ë¥¼ ê³ ë ¤í•œë‹¤.

ì´ëŸ¬í•œ í”„ë¡œê·¸ë¨ë³„ ì •ë³´ë¥¼ í†µí•©í•˜ê¸° ìœ„í•´ ë¨¼ì € ì½”ë“œë¥¼ ì¼ë ¨ì˜ ê·œì¹™ìœ¼ë¡œ í‘œí˜„í•œ ë‹¤ìŒ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ê·œì¹™ì„ ì¸ì½”ë”©í•˜ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ íŠ¸ë¦¬ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë…¸ë“œì˜ ì¸ì½”ë”©ëœ í‘œí˜„ì„ ì¡°ìƒê³¼ ê²°í•©í•©ë‹ˆë‹¤.



### 6. AST Representation

![production-rule](production-rule.png)

**Rule Sequence Embedding**

- ê·œì¹™ ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ê¸° ìœ„í•´ì„œ ê·œì¹™ IDë¥¼ ì‚¬ìš©í•œë‹¤.
- ê·œì¹™ë“¤ì„ table-lookup ì„ë² ë”©ìœ¼ë¡œ real-valued vectorsë¡œ ë‚˜íƒ€ë‚¸ë‹¤.

**Rule Definition Encoding**

- ìœ„ì˜ í…Œì´ë¸” ì¡°íšŒ ì„ë² ë”©ì€ ë¬¸ë²• ê·œì¹™ì„ ì›ì í† í°ìœ¼ë¡œ ì·¨ê¸‰í•˜ê³  ê·œì¹™ ë‚´ìš©ì˜ ì •ë³´ë¥¼ ìƒìŠµë‹ˆë‹¤.
- ì´ ë¬¸ì œë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ ê·œì¹™ ì •ì˜ì˜ ì¸ì½”ë”©ìœ¼ë¡œ ê·œì¹™ í‘œí˜„ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

**Position and Depth Embeddings**

Position embedding

![4.png](4.png)





### 7. Neural Structure of AST Reader

four sub-layers (self-attention, gating mechanism, NL attention, tree convolution)

residual connection except the layer of tree convolution

layer normalization



**Self-Attention**

Transformer-like self-attention layer

input : sum of the rule embedding, position embedding, dept embedding

extract features

**Gating Mechanism**

Content-encoding rule representation

Transformer-extracted features

**NL Attention**

Multi-head NL attention (similar to the Transformer decoder's attention to its encoder)

**Tree Convolution**



### 8. Decoder





### 9. Training and Inference



 

























