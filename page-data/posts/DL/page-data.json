{"componentChunkName":"component---src-templates-category-template-js","path":"/posts/DL","result":{"pageContext":{"currentCategory":"DL","categories":["All","bible","APR","featured","CodeGeneration","bug","블로그","DL","bytecode"],"edges":[{"node":{"id":"d753440e-354a-5233-ad8b-5464f524dd69","excerpt":"Summary about Deep learning lecture Originally from Youtube lecture Activation function 주요 활성화 함수 logistic 함수 (Sigmoid function) : 요즘은 잘 안씀 Hyperblolic Tangent (tanh) : 텍스트, sequence data, 다른 분야에선 ㄴㄴ Rectified Linear Unit (Relu) : 이미지, 주로 많이 사용됨 Leaky Relu Exponential linear unit(ELU) 경사손실 문제 : 은닉층의 한 활성함수값이 0에 가까워지면 다른 은닉층의 값에 상관없이 0에 가까워지는 문제 -> 은닉층이 많을 수록 발생할 확률이 커져서 많이 쌓기 어렵다 -> Relu가 많이 사용되는 이유 Optimizer 주로 Adam, RMSprop, Adadelta가 사용됨 Momentum : 이전 업데이트 정보를 기억, 현재 업데이트에 반영하는 방법 장점 (GD에 비해…","fields":{"slug":"/Deep-learning/"},"frontmatter":{"categories":"DL","title":"Deep learning","date":"July 22, 2023"}},"next":{"fields":{"slug":"/Uniapr/"}},"previous":{"fields":{"slug":"/index-in-blog-error/"}}}]}},"staticQueryHashes":["1073350324","1956554647","2938748437"]}