{"componentChunkName":"component---src-templates-blog-template-js","path":"/index-in-blog-error/","result":{"data":{"cur":{"id":"0f638ce2-927e-57c1-bc06-ed62b112bed0","html":"<p><a href=\"https://dolmaengc.github.io/gatsby-starter-zoomkoding-introduction/\">blog</a>에 의하면 목록을 생성하려면\nindex.md 파일 마지막에</p>\n<p>이걸 추가하라해서 추가하니깐 404 error 발생</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">```toc```</code></pre></div>\n<p>-></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"># 제목 1\n## 제목 2\n### 제목 3\n#### 제목 4</code></pre></div>\n<h1 id=\"제목-1\" style=\"position:relative;\"><a href=\"#%EC%A0%9C%EB%AA%A9-1\" aria-label=\"제목 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>제목 1</h1>\n<h2 id=\"제목-2\" style=\"position:relative;\"><a href=\"#%EC%A0%9C%EB%AA%A9-2\" aria-label=\"제목 2 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>제목 2</h2>\n<h3 id=\"제목-3\" style=\"position:relative;\"><a href=\"#%EC%A0%9C%EB%AA%A9-3\" aria-label=\"제목 3 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>제목 3</h3>\n<h4 id=\"제목-4\" style=\"position:relative;\"><a href=\"#%EC%A0%9C%EB%AA%A9-4\" aria-label=\"제목 4 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>제목 4</h4>\n<p>toc이 목록을 만들어주는데 #으로 만든 제목 기준인거 같다.\n그래서 #없는 index.md 파일에 넣으면 post 페이지 전체가 404 error 뜨는거였음ㅋㅋ</p>","excerpt":"blog에 의하면 목록을 생성하려면\nindex.md 파일 마지막에 이걸 추가하라해서 추가하니깐 404 error 발생 -> 제목 1 제목 2 제목 3 제목 4 toc이 목록을 만들어주는데 #으로 만든 제목 기준인거 같다.\n그래서 #없는 index.md 파일에 넣으면 post 페이지 전체가 404 error 뜨는거였음ㅋㅋ","frontmatter":{"date":"July 23, 2023","title":"ERROR 85901  GRAPHQL 목록 기능 추가 중 404 에러","categories":"blog","author":"DolmaengC","emoji":"🧢"},"fields":{"slug":"/index-in-blog-error/"}},"next":{"id":"d753440e-354a-5233-ad8b-5464f524dd69","html":"<p>Summary about Deep learning lecture</p>\n<p><em>Originally from <a href=\"https://www.youtube.com/playlist?list=PLAudaEp1AjCF3EPZ8ZP8R3qy0iABN8uNN\">Youtube lecture</a></em></p>\n<hr>\n<h4 id=\"activation-function\" style=\"position:relative;\"><a href=\"#activation-function\" aria-label=\"activation function permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Activation function</h4>\n<p>주요 활성화 함수</p>\n<ul>\n<li>logistic 함수 (Sigmoid function) : 요즘은 잘 안씀</li>\n<li>Hyperblolic Tangent (tanh) : 텍스트, sequence data, 다른 분야에선 ㄴㄴ</li>\n<li>Rectified Linear Unit (Relu) : 이미지, 주로 많이 사용됨</li>\n<li>Leaky Relu</li>\n<li>Exponential linear unit(ELU)</li>\n</ul>\n<p>경사손실 문제 : 은닉층의 한 활성함수값이 0에 가까워지면 다른 은닉층의 값에 상관없이 0에 가까워지는 문제 -> 은닉층이 많을 수록 발생할 확률이 커져서 많이 쌓기 어렵다 -> Relu가 많이 사용되는 이유</p>\n<h2 id=\"optimizer\" style=\"position:relative;\"><a href=\"#optimizer\" aria-label=\"optimizer permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Optimizer</h2>\n<p>주로 Adam, RMSprop, Adadelta가 사용됨</p>\n<ul>\n<li>\n<p>Momentum : 이전 업데이트 정보를 기억, 현재 업데이트에 반영하는 방법</p>\n<ul>\n<li>장점 (GD에 비해서)\n<ul>\n<li>local minimum을 잘 피한다.</li>\n<li>수렴하는 속도가 빠르다.</li>\n</ul>\n</li>\n<li>주로 다른 기법과 같이 사용됨</li>\n</ul>\n</li>\n<li>\n<p>Adagrad (Adaptive Gradient)</p>\n<ul>\n<li>GD -> 업데이트 횟수와 상관없이 learning rate를 동일하게</li>\n<li>하지만, Adagrad는 다르게\n<ul>\n<li>지금까지 업데이트 된 정도를 반영한다.</li>\n<li>지금까지 업데이트가 많이된 파라미터는 learning rate를 작게</li>\n</ul>\n</li>\n<li>주요 문제\n<ul>\n<li>GT가 갈수록 커진다. -> 업데이트가 거의 발생하지 않는다. -> 최소지점에 도착하기 전에 업데이트가 멈출 수 있다.</li>\n<li>Adadelta\n<ul>\n<li>무조건적으로 줄어드는 learning rate 문제를 보완</li>\n<li></li>\n</ul>\n</li>\n<li>RMSprop\n<ul>\n<li>무조건적으로 줄어드는 learning rate문제를 보완</li>\n<li>합이 아니라 평균을 사용</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Adam</p>\n<ul>\n<li>RMSprop (or Adadelta) + momentum</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"가중치-초기화\" style=\"position:relative;\"><a href=\"#%EA%B0%80%EC%A4%91%EC%B9%98-%EC%B4%88%EA%B8%B0%ED%99%94\" aria-label=\"가중치 초기화 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>가중치 초기화</h2>\n<p>Random Value로 초기화</p>\n<ul>\n<li>\n<p>활성화 함수 : Sigmoid or tanh 인 경우</p>\n<ul>\n<li>\n<p>Xavier Weight Initialization : uniform 분포하는  기법</p>\n<ul>\n<li>\n<p>일반적으로 많이 쓰임</p>\n</li>\n<li>\n<p>Keras에서 기본신경망의 경우, kernel_initilization=‘glory_uniform’로 설정되어 있음</p>\n<p>->Xavier 방법</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Normalized Xavier Weight Initialization : 첫번째랑 비슷한데 구간을 나눔, 잘 안쓰임</p>\n</li>\n</ul>\n</li>\n<li>\n<p>활성화 함수 : relu</p>\n<ul>\n<li>He Weight Initialization\n<ul>\n<li>주로 이미지 분석(CNN 알고리즘)에서 많이 사용됨</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"object-detection\" style=\"position:relative;\"><a href=\"#object-detection\" aria-label=\"object detection permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Object detection</h1>\n<p>1단계 : 물체가 있을 만한 경계상자(regions of interest, ROI) 추출</p>\n<p>2단계 : 추출된 ROI를 이용하여 localization과 classification을 수행</p>\n<h2 id=\"one-stage-detectors\" style=\"position:relative;\"><a href=\"#one-stage-detectors\" aria-label=\"one stage detectors permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>One stage detectors</h2>\n<ul>\n<li>앞의 두 단계를 한번에 수행</li>\n<li>SSD, YOLO 등</li>\n</ul>\n<h2 id=\"two-stage-detectors\" style=\"position:relative;\"><a href=\"#two-stage-detectors\" aria-label=\"two stage detectors permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TWO stage detectors</h2>\n<ul>\n<li>앞의 두 단계를 구분해서 수행</li>\n<li>R-CNN family</li>\n<li>속도가 느리다는 단점</li>\n</ul>\n<h2 id=\"ssd\" style=\"position:relative;\"><a href=\"#ssd\" aria-label=\"ssd permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SSD</h2>\n<p>Feagure map 추출</p>\n<ul>\n<li>Object detection을 위해서 사용하는 feature map의 수 6개\n<ul>\n<li>더 많은 feature map이 되출되지만 그중 특정 6개만 사용</li>\n</ul>\n</li>\n<li>1차적으로 VGG16과 같은 사전학습 모형을 사용하여 feature map 추출, 이후 추가적인 convolutional layer를 사용하여 6개의 feature map 추출\n<ul>\n<li>VGG16dl cncnfgks feature map 중 1개 사용 + 추가 convolutional layer가 추출한 6개중 5개 사용\n<ul>\n<li>Feature map 마다의 크기가 다음</li>\n</ul>\n</li>\n<li>Multi-scale object detection\n<ul>\n<li>크기가 다른 여라개의 feature map을 사용하여 물체 탐지</li>\n<li>다양한 크기의 물체를 잘 찾을 수 있음</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Non-maximum suppression\n<ul>\n<li>확률이 가장 큰 anchor box를 선택</li>\n<li>특정 클래서에 대해서\n<ul>\n<li>확룰이 특정값 (예, 0.1) 이하인 상자는 모두 삭제</li>\n<li>남아있는 상자들에 대해서 확률값이 제일 큰 상자를 선택\n<ul>\n<li>해당 상자와 IoU 값이 0.45 이상인 다른 상자들을 모두 삭제</li>\n</ul>\n</li>\n<li>남아 있는 상자들에 대해서 동일한 과정 반복\n<ul>\n<li>확률값이 제일 큰 상자를 선택 -> 해당 상자와 IoU 값이 0.45 이상인 다른 상자들을 모두 삭제</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"r-cnn-family\" style=\"position:relative;\"><a href=\"#r-cnn-family\" aria-label=\"r cnn family permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>R-CNN family</h2>\n<p>R-CNN in 2014, Fast R-CNN in 2015, Faster R-CNN in 2016</p>\n<ol>\n<li>Input images</li>\n<li>Extract region proposals(~2k)</li>\n<li>Compute CNN features</li>\n<li>Classify regions</li>\n</ol>\n<ul>\n<li>크게 4부분으로 구성\n<ul>\n<li>Extract regions of interest(ROIs)\n<ul>\n<li>Selective search (최근엔 사용하지 않음)</li>\n</ul>\n</li>\n<li>Feature extraction module\n<ul>\n<li>이전 단계에서 추출된 ROI들에 대해 CNN을 적용해서 feature map 추출</li>\n</ul>\n</li>\n<li>Classification module\n<ul>\n<li>이전 단계에서 추출된 feature들에 대해서 classification 예측</li>\n<li>Support vector machine (SVM) 사용</li>\n</ul>\n</li>\n<li>Localization module\n<ul>\n<li>ROI를 기준으로 GTBB에 대한 offset을 예측함</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>R-CNN의 주요 단점\n<ul>\n<li>계산량이 많아 시간이 오래 걸린다.</li>\n</ul>\n</li>\n<li>Fast R-CNN\n<ul>\n<li>사전 학습모형을 먼저 적용하여 feature map 추출-> 그 다음에 selective search 방법 적용하여 ROI 추출</li>\n<li>SVM 대신 fully connected layer 사용</li>\n<li>단점 : 여전히 selective search 방법을 사용해서 속도가 느림</li>\n</ul>\n</li>\n<li>Faster R-CNN\n<ul>\n<li>Region Proposal Network 방식 사용\n<ul>\n<li>VGG16 등의 사전학습모형을 사용하여 feature map 추출</li>\n<li>Feature map의 각 셀에 대해서 크기와 형태가 다른 9개의 anchor box 생성</li>\n<li>각 anchor box에 대해서 두 가지 종류의 값들 예측\n<ul>\n<li>Objectness score : 물체가 있을 확률</li>\n<li>GTBB와의 offset 값들</li>\n</ul>\n</li>\n<li>Anchor box에 예측된 offset을 적용하여 ROIs를 추출</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"rnn-recurrent-neural-networks\" style=\"position:relative;\"><a href=\"#rnn-recurrent-neural-networks\" aria-label=\"rnn recurrent neural networks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RNN (Recurrent Neural Networks)</h2>\n<ul>\n<li>Sequence data를 다루기에 적합</li>\n<li>텍스트 데이터 분석에 적합\n<ul>\n<li>단어들의 문맥적 의미 추출에 용이</li>\n<li>단어들 간의 관계 정보 추출에 용이</li>\n<li>어떠한 단어들이 어떠한 순서로 언제 사용되었는지에 대한 정보 추출 용이</li>\n</ul>\n</li>\n<li>언어모형 (language model), 분류(예, 감성분석), 기계 번역</li>\n</ul>\n<h2 id=\"lstm\" style=\"position:relative;\"><a href=\"#lstm\" aria-label=\"lstm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>LSTM</h2>\n<ul>\n<li>RNN 기반의 신경망 알고리즘</li>\n<li>Simple RNN의 문제를 보완하기 위해 제안\n<ul>\n<li>Problem of long term dependency\n<ul>\n<li>입려괸 문서에서 상대적으로 오래전에 사용된 단어의 정보가 잘 전달 되지 않는다.</li>\n<li>주요 원인 : 경사손실문제</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Forget 게이트\n<ul>\n<li>역할 : 이전 기억 셀 (즉, ct-1)이 가지고 있는 정보 중에서 정답을 맞히는데 불필요한 정보는 잊어버리는 역할\n<ul>\n<li>Ct-1이 가지고 있는 각 원소의 정보 중에서 몇 %를 잊어버릴 것이냐를 결정하기 위해서 0~1사이의 값을 반환하는 sigmoid 함수를 사용</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Input 게이트\n<ul>\n<li>일부의 정보가 삭제된 ct-1(즉, ct-1’)에 새로운 정보를 추가하는 역할</li>\n<li>일단, 추가하고자 하는 정보를 계산 : ht-1와 단어t의 정보를 사용</li>\n<li>새롭게 추가되는 정보들의 긍부정 역할을 나타내기 위해 -1~1의 값을 출력하는 tanh()를 사용</li>\n<li>그대로 반영되는 것이 아니라, 정답을 맞히는데 있어서 기여하는정도에 따라서 적용되는 비율을 다르게 함 -> 이를 위해 sigmoid 함수 사용</li>\n</ul>\n</li>\n<li>Output 게이트\n<ul>\n<li>Output 게이트의 역할은 forget게이트와 input 게이트를 이용하여 업데이트된 기억셀의 정보, 즉 ct,를 이용하여 ht를 계산하는 것</li>\n<li>output 게이트는 ct가 갖는 원소의 값들을 조정하여 ht를 계산</li>\n<li>ct가 갖고 있는 원소들 중에서 정답을 맞히는데 중요한 역할을 하는 원소의 비중은 크게하고, 그렇지 않은 원소들의 비중은 작게해서 ht를 계산</li>\n<li>각 원소의 비중을 계산하기 위해서 현재 LSTM층에 입력되는 ht-1과 단어t의 정보(xt) 인자로 갖는 sigmoid 함수를 사용</li>\n<li>ct원소들의 긍부정 역할을 구분하기 위해 tanh 적용</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 45.55555555555556%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABZ0lEQVQoz02RbVOCUBCF+f8/q+yL+KVCCGdSQbBmiiuQeuG+neasSTBzZt+e3WUhAgJCAGww+Pg84n27xVueoygK7Is9tNbgE0IQ+Zn1s5jirIiOdR6jNTDWwlqHYTSSs87BWDcNZMw6c/f6rdeKzycyZsQoMtCDhnUWgEcIHtYaieciS855JwzFGcYYyUXTZnfbQtgTRpA3Pl8vcN7/yYm4nIPJ8Uw9DLjo6/xkh6se5FQW2eB4krWS53fiQC7Qwzgx5Nk/jLecnHw+n9H1Hb6VQqMafDcNGqXQdq3kT+0JXdej//lB27ZSI6OUwlfToO97YdXpJH5UHkpQdV2hqg6TtrsdykOFuq7lb1fVvf7PlmWB9+0O9fEoM4pijyjLMqRphnWaIUnWSJIUyTrFy2siNs3esNlskOe5cMy9Ugm1xjO5NEP2x0Wr1QrUYvGE5XIpfhyvJhvHMe4M/YfHR7Ex41n+3vML892nZVxzrwUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"structure-LSTM.png\"\n        title=\"structure-LSTM.png\"\n        src=\"/static/3ab20e713910dcb3192f7f41d70151d8/37523/structure-LSTM.png\"\n        srcset=\"/static/3ab20e713910dcb3192f7f41d70151d8/e9ff0/structure-LSTM.png 180w,\n/static/3ab20e713910dcb3192f7f41d70151d8/f21e7/structure-LSTM.png 360w,\n/static/3ab20e713910dcb3192f7f41d70151d8/37523/structure-LSTM.png 720w,\n/static/3ab20e713910dcb3192f7f41d70151d8/302a4/structure-LSTM.png 1080w,\n/static/3ab20e713910dcb3192f7f41d70151d8/07a9c/structure-LSTM.png 1440w,\n/static/3ab20e713910dcb3192f7f41d70151d8/ab40b/structure-LSTM.png 1736w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h2 id=\"bidirectional-lstm\" style=\"position:relative;\"><a href=\"#bidirectional-lstm\" aria-label=\"bidirectional lstm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Bidirectional LSTM</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 43.888888888888886%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABlUlEQVQoz1VR266bMBDk//+pystRRRIOJE/cC5gokU6TGAzGN2AqGyVqRhrtLjueXWxvmiYINUIZAcZ6XC4XLMuCeZ7ftLXF/f4XRZ6BtARFUYAQgqqqcL1dsa6r03jW0OJlMrABwzCAcw7OJzA2QGvtNFJKMMZcnPjkaLU2WljTt+G6bBO4NPhhAi+8tnsd+B8fvWV1fBtaKG3Q3pkjHSYopSGUgpAS2hgIqTAJCWOM21IqWwtg3YZ9bGihzQw6SlCuwJXBPBuYeYY0211aI+XyBdpo6FlDavPxBx6lFMpuIQSEmGAHUNrhQTuwkaNnA+6PJzo2gI0jHk+KZ9eDcY6u77de17u7tFt7WZahrmv3YnVDUNU1qoagJgR/qtp9s3nVNFtNCJq2dbnVkfaC2/WGsijdy3vn8xlpmjomSeKY5gWyLHd5nKbI8tz14yRGmmWuZ/MkTVGWBcrC6jPEcQzP932cTieEYeiiv/ex2+0QfoeIwgj7/R5BEOAURTgGRxwOB0RR5Lj7+o1fXz6OwTeicNP/Ay48pg2VT1uqAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"bidirectional-LSTM.png\"\n        title=\"bidirectional-LSTM.png\"\n        src=\"/static/52caa385798048f706c0ab98dedf03cc/37523/bidirectional-LSTM.png\"\n        srcset=\"/static/52caa385798048f706c0ab98dedf03cc/e9ff0/bidirectional-LSTM.png 180w,\n/static/52caa385798048f706c0ab98dedf03cc/f21e7/bidirectional-LSTM.png 360w,\n/static/52caa385798048f706c0ab98dedf03cc/37523/bidirectional-LSTM.png 720w,\n/static/52caa385798048f706c0ab98dedf03cc/302a4/bidirectional-LSTM.png 1080w,\n/static/52caa385798048f706c0ab98dedf03cc/07a9c/bidirectional-LSTM.png 1440w,\n/static/52caa385798048f706c0ab98dedf03cc/701e9/bidirectional-LSTM.png 1780w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h2 id=\"gru\" style=\"position:relative;\"><a href=\"#gru\" aria-label=\"gru permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GRU</h2>\n<ul>\n<li>\n<p>LSTM보다 성능이 떨어져서 잘 사용되지 않음</p>\n</li>\n<li>\n<p>게이트 개념 사용</p>\n<ul>\n<li>\n<p>기억셀을 사용하지 않음</p>\n</li>\n<li>\n<p>Rest 게이트와 update 게이트</p>\n<ul>\n<li>\n<p>hidden state 정보를 업데이트하기 위해서 reset 게이트와 update 게이트 사용</p>\n</li>\n<li>\n<p>GRU는 LSTM 보다는 간단한 구조</p>\n<ul>\n<li>속도는 빠느라 정확도가 떨어짐</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 44.99999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABvUlEQVQoz32RW2/aQBCF/f9/Q9X2pSppC1WSotStAXMJ5eLYhiRQlGDaBCywC0ZgB4NtvmodBbUvjDSas2d2zszsSvv9HmFBuEI3dO6HQyzLYjKZpLzIh3HCMdtsQhzXTbH0QsZJxOZpw3a7JdpFRFGc8qvNjvVTiD13+bOycbwJM2/MYj1NsbeeMpn+xjCMZ8EkSYiiZzGBd9sdcRSnokmcEIQRnZFDrpJHbmRoDmQMq4Baz6INvnOpn/HDlGlrbXw/QAqCAM/zGA6HaRSruq7LfD4n3IRp1zCB6weNunmOUsthhwPUymfU1hlyNUv/UTusL/m+z3K5ZDwep3GxWKQuBEUzwQnrPbRp3Mp8q3+g0j2lrGepXedptt5g9i5IxF/sQRKFLxPZtp1iISgmFROLhuIJjIHKR/kVJ/Jb3n99zafSO4rmOeWrE7pWmSDw040kx3H412ez2X9n13WYuXNyapFMPkOldcGlrtDslmiYRbR+jbvHm0OdJN7umFvWkMHdPV+qOtWWzs9+H8saMRr9ot/rYxomtze9w32p0+lwzE3TpNvtcqW1KSgKxWIRXdfTnFpSURQFpaAcuL/zIIyUnlfzWAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"structure-GRU.png\"\n        title=\"structure-GRU.png\"\n        src=\"/static/013dab06a3e80506b2ba415e54d09d5c/37523/structure-GRU.png\"\n        srcset=\"/static/013dab06a3e80506b2ba415e54d09d5c/e9ff0/structure-GRU.png 180w,\n/static/013dab06a3e80506b2ba415e54d09d5c/f21e7/structure-GRU.png 360w,\n/static/013dab06a3e80506b2ba415e54d09d5c/37523/structure-GRU.png 720w,\n/static/013dab06a3e80506b2ba415e54d09d5c/302a4/structure-GRU.png 1080w,\n/static/013dab06a3e80506b2ba415e54d09d5c/07a9c/structure-GRU.png 1440w,\n/static/013dab06a3e80506b2ba415e54d09d5c/9eaa0/structure-GRU.png 1676w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h2 id=\"seq2seq\" style=\"position:relative;\"><a href=\"#seq2seq\" aria-label=\"seq2seq permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>seq2seq</h2>\n<ul>\n<li>\n<p>주요 목적</p>\n<ul>\n<li>To convert sequences from one domain to sequences in anotjer domain, 번역이 대표적인 예</li>\n<li>Encoder-decoder 구조</li>\n<li>Encoder의 역할\n<ul>\n<li>입력뙨 텍스트 데이터를 숫자 형태로 혹은 벡터 형태로 변환</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 42.22222222222223%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABYlAAAWJQFJUiTwAAABWUlEQVQoz5WS226DMAyGef8XG2ylLTedRA4cBgRpDEiAICjwT3HX3e1ikT6wE/92LMc7jgNu2cEiiiKcwhDn8xlZliEIAlRKkZ3nOcqyhJASXAgopVDXNZIkIb3L4/CeCXEARVFAJhJCCHw2DQV3XYf/LM991nUl4f1+/z0Yx5H2n9WHYYAxBtu20d48z3RLR1GWqFSFXvePhMuyYLIjJjuRYLtvsLOFtRMVOfaDbDtPWNeFCuz7o0VrLRXe9520nruJGUZ8tS063UMbDT0YtF2PtuvQG/1AazRtC20MzKAxTROapqHOXFLnOzxV1yjKClxK5B8FDcG1IJMEaZahrCry0yyHkAnKSkHVCmma4nZ7p78bzhNPcA7GOGLOEDMGzjnBOEfM4h9f0JmLYYxBiEeMlIIG+NQ4vMvlguv1Ct9/QRiGZEfRFYHv4+3tlXxHeDrRM3K20/zFN+/XVOfklLZqAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"encoder.png\"\n        title=\"encoder.png\"\n        src=\"/static/7f69e2dfb6829417d0964d967a630e35/37523/encoder.png\"\n        srcset=\"/static/7f69e2dfb6829417d0964d967a630e35/e9ff0/encoder.png 180w,\n/static/7f69e2dfb6829417d0964d967a630e35/f21e7/encoder.png 360w,\n/static/7f69e2dfb6829417d0964d967a630e35/37523/encoder.png 720w,\n/static/7f69e2dfb6829417d0964d967a630e35/302a4/encoder.png 1080w,\n/static/7f69e2dfb6829417d0964d967a630e35/07a9c/encoder.png 1440w,\n/static/7f69e2dfb6829417d0964d967a630e35/1acf3/encoder.png 1596w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>Decoder의 역할\n<ul>\n<li>Encoder에 의해 숫자로 변경된 정보를 다른 형태의 텍스트 데이터로 변환</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 58.333333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAAByklEQVQoz11Ty5IbIQz0//9dDrn5YHs3qcQ7bxAPSdApaWa8rlBFgQBJrVZzaV3wNTxxvV5xfzxwu98wLzNs9N7RWkdvzffnbO828LJtXHpvUBVwrRARRApoIuiq/oC5QrgCTS0DtClKThbl9aaU7EE9IN6GtuYOtjZbtUFUfTZt7sQikOPOkJ/3Yve9G8IdqpWWcsE8zyiVoeaoDcuyIBJBRKGivp+XBSyWRFFZ3IdSfg+482FOKSdHYY8NaS4FuVZHZ7MwI+XsyXrb0ZldWDzG5ST3HCcXzpl35rDtzVtj/Ojg8OXzP4cxBszz6PCtpFwqnsOINQQv1xytXKOhqXhltRRM0+CNMU4viRtGYs+6JsYQM8aQsFDBkiomqhhj3m1i349UjrOKKRbMVLCmunP482PAj9sfPGbCShnTsrp8VKrLKYaAUiqYGZWr87VuGzKzo6LKeE4LPqcAqoLLEDLuQ8CWLXvBSgmF1TudWLCljJgLYt7RfIWMvys5ckNt63MjzHQgfOfQOmUBYxEvwZJMMWPNDG67tEwBbLqzPcuuWeXvphARPn/9xjCM/ivCtrkGz07XnJCI8PG4e/lwkcu3AlT93AIbVf8AAGKqfuOwmC4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"decoder.png\"\n        title=\"decoder.png\"\n        src=\"/static/d8d309f9e9f9a8b7dbe59c673822ebd4/37523/decoder.png\"\n        srcset=\"/static/d8d309f9e9f9a8b7dbe59c673822ebd4/e9ff0/decoder.png 180w,\n/static/d8d309f9e9f9a8b7dbe59c673822ebd4/f21e7/decoder.png 360w,\n/static/d8d309f9e9f9a8b7dbe59c673822ebd4/37523/decoder.png 720w,\n/static/d8d309f9e9f9a8b7dbe59c673822ebd4/302a4/decoder.png 1080w,\n/static/d8d309f9e9f9a8b7dbe59c673822ebd4/07a9c/decoder.png 1440w,\n/static/d8d309f9e9f9a8b7dbe59c673822ebd4/277c6/decoder.png 2522w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>Encoder와 Decoder를 위해 순환신경망 기반 모형 사용 가능 (예, RNN)\n<ul>\n<li>첫번째 RNN이 encoder 역할, 두번째 RNN이 decoder 여할</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"transformer\" style=\"position:relative;\"><a href=\"#transformer\" aria-label=\"transformer permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Transformer</h2>\n<ul>\n<li>소개\n<ul>\n<li>2017년에 google에서 제안한 attention 기반의 encoder-decoder 알고리즘\n<ul>\n<li>순환신경망 기반의 방법이 아니라 attention 사용</li>\n<li>주요 applications:\n<ul>\n<li>BERT (Bidirectional Encoder Representations from Transformers)\n<ul>\n<li>encoder만 사용</li>\n<li>단어 embedding</li>\n<li>문서 embedding</li>\n<li>분류</li>\n<li>Q&#x26;A</li>\n</ul>\n</li>\n<li>GPT (Generative Pre-trained Transformer)\n<ul>\n<li>decoder만 사용</li>\n<li>생성모델</li>\n</ul>\n</li>\n<li>BART (Bidirectional and Auto-Regressive Transformers)\n<ul>\n<li>Encoder, decoder 둘 다 사용</li>\n<li>텍스트 요약</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Transfomer를 이해하기 위해서는 attention을 먼저 이해하는 것이 필요</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"attention\" style=\"position:relative;\"><a href=\"#attention\" aria-label=\"attention permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Attention</h3>\n<h4 id=\"encoder-decoder-attention\" style=\"position:relative;\"><a href=\"#encoder-decoder-attention\" aria-label=\"encoder decoder attention permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Encoder-decoder attention</h4>\n<ul>\n<li>\n<p>Why was it proposed?</p>\n<ul>\n<li>순환신경망 기반의 seq2seq 모형이 갖는 문제점을 보완하기 위해</li>\n<li>순환신경망 기반의 seq2seq의 주요한 문제점\n<ul>\n<li>입력된 sequence data에 대해서 하나의 고정된 벡터 정보(마지막 hidden state)만을 decoder로 전달한다는 것</li>\n</ul>\n</li>\n<li>그렇다면 어떻게 하면 되는가?\n<ul>\n<li>Encoder 부분에서 생성되는 각 단어에 대한 hidden state 정보를 모두 decoder로 전달</li>\n<li>벡터정보들을 쌓아서 행렬로 만들어서 전달</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>가중치의 계산</p>\n<ul>\n<li>가중치는 hs의 각 hidden state와 decoder에 예측하고자 하는 단어에 대한 hidden state와의 유사도를 가지고 계산</li>\n<li>Hidden state 간의 유사도를 계산 -> 내적 연산</li>\n<li>decoder 부분의 첫번째 RNN층에서 출력되는 ‘Today’ 단어를 예측하는데 사용되는 hidden state -> hd,0\n<ul>\n<li>hd,0 = (1 0 0 0 2)</li>\n</ul>\n</li>\n<li>h0, h1, h2와 hd,0과의 내적 연산\n<ul>\n<li>(1 0 0 1 2) *(1 0 0 0 2) = 1 + 4 = 5</li>\n<li>(1 0 0 1 1) *(1 0 0 0 2) = 1 + 2 = 3</li>\n<li>(1 0 0 0 1) *(1 0 0 0 2) = 1 + 2 = 3</li>\n<li>이 값들을 attention score라고 함\n<ul>\n<li>Attention score의 값이 클수록 관련도가 크다는 것을 의미</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Attention score를 가지고 가중치 계산</li>\n<li>가중치는 확률값으로 표현</li>\n<li>확률값을 계산하기 위해서 attention score에 softmax()를 적용</li>\n</ul>\n</li>\n<li>\n<p>최종적으로 출력되는 값</p>\n<ul>\n<li>Attention에서 출력되는 값과 RNN 층에서 출력되는 값 간의 이어붙이기(concatenation)</li>\n<li>즉, Concat((1.0 0.1 0 0.9 1.8), (1 0 0 0 2))</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"self-attention\" style=\"position:relative;\"><a href=\"#self-attention\" aria-label=\"self attention permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Self-attention</h4>\n<p>Attention과의 차이</p>\n<ul>\n<li>Attention은 encoder-decoder 모형에서 보통 decoder에서 encoder에서 넘어오는 정보에 가중치를 주는 식으로 작동</li>\n<li>Self-attention은 입력된 텍스트 데이터 내에 존재하는 단어들간의 관계를 파악하기 위해 사용\n<ul>\n<li>관련이 높은 단어에 더 많은 가중치를 주기 위해 사용</li>\n</ul>\n</li>\n<li>지시대명사가 무엇을 의미하는지 등을 파악하는데 유용</li>\n</ul>\n<p>Transformer에서의 self-attention (or attention)</p>\n<ul>\n<li>\n<p>Transformer의 self-attention은 입력 받은 단어들 중에서 어떠한 단어에 더 많은 가중치를 줘야 하는지 파악하기 위해서 각 단어들에 대한 Query, Key, Value 라고 하는 서로 다른 3개의 벡터들을 사용</p>\n<ul>\n<li>Key, Value 벡터들은 사전 형태의 데이터 의미 : Key는 단어의 id와 같은 역할, Value는 해당 단어에 대한 구체적 정보를 저장하는 역할</li>\n<li>Query 벡터는 윳한 다른 단어를 찾을 때 사용되는 (질의) 벡터라고 생각 가능</li>\n</ul>\n</li>\n<li>\n<p>작동 순서</p>\n<ul>\n<li>단계1 : 입력된 각 단어들에 대해서 Query, Key, Value 벡터를 계산\n<ul>\n<li>이떄 각각의 가중치 행렬이 사용됨</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 59.44444444444444%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAAB3UlEQVQoz4VSaW/qQAzM//9PSIjjCwFajgC5G3bJQQK5CGfCVDZN9YSe1I1GXizPeLxYudVnWJYFbbWC7/vQdQNCCqxWK84bhgEpBXRdhx8EsCwbS03Dfr8HnaZp8Hw+f6PSNDXu1zuCIMB6s4Ef+HwXQiJJDgjDCHESI04ShpASX56Hja4jivYoywJFXuBwOOB8PkN5tQGyPINhGiwohcBOSnjelslxvIdt2yzoui6iKMJWCBRFAeDlroVCNun736GCNE1xuVxQliXqusZfhwVJ73Q6MYnsn8oTsjRDQiMKwfntdssjtY0aejcy8wYe+dkAZVEiyzIm3W43jvSbROlOfxjl/3RIY9SPGtfrFVVVoTpXr1hVOB6P7JDEpJRI0yNPkuX5a5qiwP3x4KdooVAyT3PsdjsYpsmP73ke5vMZrwetiWmZWK/XWCzmsB0btu1wI8dxeIpWnKCEYYgwCnkFaDxCHMdcSATHdWGaJjtsn4BAe0h1URiBNX6g0Bo4jsvkl4vFbxwOh5jNZhgMBlBVFZqmcZ4Wnpv9gDRaKFREWC4XmH5MMZlM8PH5ifF4jG63y0KdTgf9fh/T6ZTzJNry3qGMVBWj0YiJVEygOwn3ej0GCZLbVlD9h/OOb3IkebPImWxbAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"qkv2.png\"\n        title=\"qkv2.png\"\n        src=\"/static/f3bdb97b6076487b7818a5f058b2eb37/37523/qkv2.png\"\n        srcset=\"/static/f3bdb97b6076487b7818a5f058b2eb37/e9ff0/qkv2.png 180w,\n/static/f3bdb97b6076487b7818a5f058b2eb37/f21e7/qkv2.png 360w,\n/static/f3bdb97b6076487b7818a5f058b2eb37/37523/qkv2.png 720w,\n/static/f3bdb97b6076487b7818a5f058b2eb37/302a4/qkv2.png 1080w,\n/static/f3bdb97b6076487b7818a5f058b2eb37/3c492/qkv2.png 1300w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 54.44444444444444%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB7klEQVQoz51SaY+iQBDl//8WM4nGI973HWM8QFBgVFRuRo6JRoG36XLH7JdNNtvh0UV19XtFVXFJ+oTjWhB4AYfDAYqiYCfLkKQtNpsNjtoRm40A1/PwfD4RxzGSJMHfFpciBXu+vC+IkgjDNKGqKvaHPURJwna7Bc8L5Fvza8iyDEVVYds2brcboihCEAS43+94PB7g0jQl5iiMsFgucdF1fH7u4XkeHMeBaZpwXZe+fd+H47qwLAuWZcM0DDpnd0zTovM3oX/1wQs8dvKOVP93cfROQfVxXQeyosD4rfyT3fd3BCb8L+B+ipwkMfzgSoQ8z2OxWFCDNE2D49hUnyRO3k1he/yMX/sf4MIwpIx0XYdlWjidTnBsB57rUYbn8/lVM9NCFITUvMtFpxoaug7bshEGIYJrgMAPwDGy9WoNQRAgSRJlpx01IlgulzQ6DMw+n05QFZVsFivwPERRhKEbuJwvBI79VqfTwXg8xmQyQbfbxWq1ojFpNptvf7vdJtH5fE4xs9kMw+EQg8HgNbvbLXa7HTimVi6X0Wg00Gq1yB6NRpRFPp9HrVZDvV5HoVDAdDolsmKxSGKVSgXVapVimRADxxQymQxyuRzh4+ODCJg/m83S5VKpRDbLslatURwTZoJMqN/vk1Cv18Mv6EwhhF2+5HUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"qkv.png\"\n        title=\"qkv.png\"\n        src=\"/static/2746185ebf44f126293e0728a4b0bb6b/37523/qkv.png\"\n        srcset=\"/static/2746185ebf44f126293e0728a4b0bb6b/e9ff0/qkv.png 180w,\n/static/2746185ebf44f126293e0728a4b0bb6b/f21e7/qkv.png 360w,\n/static/2746185ebf44f126293e0728a4b0bb6b/37523/qkv.png 720w,\n/static/2746185ebf44f126293e0728a4b0bb6b/302a4/qkv.png 1080w,\n/static/2746185ebf44f126293e0728a4b0bb6b/07a9c/qkv.png 1440w,\n/static/2746185ebf44f126293e0728a4b0bb6b/68638/qkv.png 1510w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>\n<p>단계2 : Attention score 계산</p>\n<ul>\n<li>Query를 이용하여 각 Key들하고의 유사한 정도를 계산 -> 내적 연산</li>\n</ul>\n</li>\n<li>\n<p>단계3 : Attention score를 이용하여 가중치 계산</p>\n<ul>\n<li>Softmax() 함수를 적용</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 58.88888888888889%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAAB7klEQVQoz3WSa3OaQBSG+f/fk8y0k+SPVGsMRkVsLAqiqNxBuehALCoob2ePZaqt2ZlndoE9Dy9n4XaHDKZhYqxOYJgGFvoCnufDcRwsl0vYjgPLtmDbNt0rigLV2O12WK1W2Gw2iMIIcRyDK8sTjvmJNjuug4VhYKppMC0Luq7D8z2EYYgkSVCWJYnYXK1d10UQBCTOsgwcSuBUnOC6DmzHhm4YUKcTSjSfz2kjE7LNl7JK6Ps+JWTS7XbLEpYk9DyXEjLhbDGHZVkwDINkcRRdCS9nljxNUpLu9/u/Qt/z4Dg2VkGAPM9xOByQFzmtWd+Ox+NVurPwWs4GV10UxxxhyHoRUF9YsvV6TW9O05SSrNcxkjRB+vHxn7yCYyfFyHa/4PkufSrrHeuhaZqwLYuEtCfLiKrmFlwURYijGLO5huFQgjqZYDAYYDab0cmzX4k1nKWtYDWfwbEUhm5A7Iuo1b6hXq+D53mIoghZUTCdTqEoCjRNg6qqGI/HlPwzuLEsYywrmKgKup0OGvU6Xhrfaf3SaIBvNtFutfDz/R2iIKDbbuNccxtOkIYQJOkPI/D9H6i322j2BLyKImpvb+hKEnqjESHKMs41t+H6z8+oEB6f0PnyFa93d+Dv79F6eCB6j4/0XHw6c1nzL78BCMNr8OSpl4sAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"attention-score.png\"\n        title=\"attention-score.png\"\n        src=\"/static/f6db63e31a7538f726cfc0eb32b45792/37523/attention-score.png\"\n        srcset=\"/static/f6db63e31a7538f726cfc0eb32b45792/e9ff0/attention-score.png 180w,\n/static/f6db63e31a7538f726cfc0eb32b45792/f21e7/attention-score.png 360w,\n/static/f6db63e31a7538f726cfc0eb32b45792/37523/attention-score.png 720w,\n/static/f6db63e31a7538f726cfc0eb32b45792/302a4/attention-score.png 1080w,\n/static/f6db63e31a7538f726cfc0eb32b45792/0c3d0/attention-score.png 1414w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>단계4 : 가중치를 Value 벡터에 곱한다.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 50%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAABxUlEQVQoz2WSe2+qQBDF+f7fiPtHzVXAqJemYECrFZGXKI/dUuKbPc3MLUkTSYZ9zf7m7NnVro8z0iyFv1ggDEMsF0tsPj4QBFt4no9dFMHzPByPRygAStEfuN/vkFLi66tF03xCfkpcr1doSnVQD4WiKLENQ6RpysDdboc4SSCERFlWOJ/PDCJgDz0cjijLEmVZoCgKdF0HTVHdDgxcr9dIkhSr1QrBNuACVVXzprZtn4CksK5r5HmOqqp4joGqUxC1YFW0GGw2yLIMcZzwfH444HK5PAGFEAwkddRnICc9FKSQvJESSFHTNNjv90/qfre32419o6D+f4VKobt37BUBhZSo6gpSCgbSUXr/yKNe4W+l/Udj7XQ64dSeEQZbuK7L/v2zbURRhOXyHav1mr3N8wOroHw6PhXpW2b8hEaXUVcC+yyDO3fgOA7eHIchvu/DcVy47pyLka+kOo5jfg1RHCFNEraFfKTQdlHIgOlsAsM0MB6PMRqNMJlMYFoWDMOA/WrznGmaGA7/YjabwfpZC4KAL7MP7W1uYzgcQtd1GKaJ6XQK/Y+OwWAAyxrzGhWhzQR8GbxwjmkaDCWL6OHTaSi+AThP5Vf6m+2WAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"value-vector.png\"\n        title=\"value-vector.png\"\n        src=\"/static/d57c7cb98d48e607435faece4d3968e2/37523/value-vector.png\"\n        srcset=\"/static/d57c7cb98d48e607435faece4d3968e2/e9ff0/value-vector.png 180w,\n/static/d57c7cb98d48e607435faece4d3968e2/f21e7/value-vector.png 360w,\n/static/d57c7cb98d48e607435faece4d3968e2/37523/value-vector.png 720w,\n/static/d57c7cb98d48e607435faece4d3968e2/302a4/value-vector.png 1080w,\n/static/d57c7cb98d48e607435faece4d3968e2/4a00e/value-vector.png 1406w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>최종 결과물\n<ul>\n<li>가중치가 곱해진 value vector들의 합</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 46.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB4ElEQVQoz12P224SURSGeTojQQXra+hD9MpqDGqxvTfpnRcmemFNmwqFDoeCjDMDTLHDcBgqIAjUDseZ+cwebEB38u+V9a8/3147AOC4CwbXHTSthK7rVKtVFEUhn8/jeh6bx/uv3/REDYjL81yWzoL2lcXgVx/Xc3w5ruP3hnFJv//TzwktlnM6nR80mnVms6lAseL8Bd6e0WjKfOr+8/p86mHbDvOJs95oCVN7yXA4YTlbbydQAdH0fpu0hiqtcYXWqExrVKI1LGFdl7myddo3F3TsC6xxeeWPylgiO65gjTQGN9b6y67nclB4zO5ZkD0pwhsp7Gs/EyZ6EmLn012efw7y7DDI68R99tMR9qSwr30pwqvUHd6r22ugbdu8TT/h5Zd7xOJb7MYf+oqdbvHi8AFPP4TY+Rjya/QoTCyxkYk/InoS5F1hm+lkjmAFut0uel1GNdIotRxaLYtmZFCNDCUzS6meQzFzVBqrmfDFXDNzfl41JL43NHrdHoIVMAyDhtmiYVpoSoWqbtCst2mali/TaHJZrdOorTJiJvIlVUdVKr5XN5sIjlBAlmWKxSLZXJbj4yOSyVPO8+fIcpHC1zypVBIpLZE6S1Eo5JG/yWSyGRKJuJ/PZNMIxq3+AMqKdqUEv+VqAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"self-attention.png\"\n        title=\"self-attention.png\"\n        src=\"/static/6c6e5443b05fe0774ce86856af0d64ed/37523/self-attention.png\"\n        srcset=\"/static/6c6e5443b05fe0774ce86856af0d64ed/e9ff0/self-attention.png 180w,\n/static/6c6e5443b05fe0774ce86856af0d64ed/f21e7/self-attention.png 360w,\n/static/6c6e5443b05fe0774ce86856af0d64ed/37523/self-attention.png 720w,\n/static/6c6e5443b05fe0774ce86856af0d64ed/302a4/self-attention.png 1080w,\n/static/6c6e5443b05fe0774ce86856af0d64ed/07a9c/self-attention.png 1440w,\n/static/6c6e5443b05fe0774ce86856af0d64ed/6e29b/self-attention.png 1610w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3 id=\"transformer-1\" style=\"position:relative;\"><a href=\"#transformer-1\" aria-label=\"transformer 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Transformer</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.77777777777777%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABYlAAAWJQFJUiTwAAAC6UlEQVQ4y5WSS3PiVhCF+eHZ5w9kMVkniyQbp2JT5mmCMQ/zlpAAvZB4SGABA4M9AwjEl7rXj8WkskhXnXtvVbda5/TpxOVy4Xy+IO44/m+I/PcQ8e3rN47HI3EcczqdSPA/4vtmh+OBTq+NNTLZbrf4vk/CHe/463rC5/WRwSDAdVco6gxNC7CsEF0PGLkrzuf4Xz94fnmhUCrT6qrs93sOhwMJT/vCDz9+wpyaGIMnOt0xpQeLZstD132qNQdFmcoG6/Wa+XyO78+IooggmHF9/Qf5/C227ciaxOmw56b6M8a0yymK2B9euFyiNw6C1UUifpMq5nQ8nmTW8qb8kinzZ6nJ0DBfG+6jrziOi64PGLku/cEA1/VYrdcco+jDFBH7wwHHHtKsF9F6bYZDnbvcFdn0FYYxfGN4jjAMg16vx2g0wjRNNpuNxPvw3+9lGFJ8bPBrKsdNsYLSaKAWfmNQvWE46EunE+c4YvN5g2VZOI6D67qEy1DOSmAymchhi1itlqj9IfmqgmZO0DSNQiHJXT4piUiGclLnGMe26Wka/X4fXdfQ+zpdRWG+WAiKsjgIQmrFLPfXn+hUknhDlcfrn1CLv2O+SxaHGPR4PMa2bclSsF0ul+x2O07nE3EMp+jCwp8zbNVo5a8w21XGtoFazdKrF+V3r6bs9zzvnlEUBWc0IggCKTUIfCaTKb6/4Mt6wyZ8YuxYdNsdmq0u4WrNZrvFdDyq9SaWZROGIQlxiCbC4dlsJrc9XC5ZLEL8yYz5eMyTP8OfThmNXLROh2o6xVBRCGYzHnNZHgt3WKYpeySEs0JisVjkJpkklU5TLT1Q+7tIr9vGcWxsx5HsvbGHrqrcZzK063Usw6Ccz9Mol+WGeJ5HQkgVbjUaDUqlEpVKhW67hdLtovZ6cp3eoaqqNK7V6aBpusyXHh6o1mof+cT9/T2pVIpcLvcBwTKbzZLJZCTS6fTHW9Sm06+4vb0lKVSlXt8C/wDCFz2RrZ/QeQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"transformer.png\"\n        title=\"transformer.png\"\n        src=\"/static/7f0926aff76fe3841bfae805f26e178e/37523/transformer.png\"\n        srcset=\"/static/7f0926aff76fe3841bfae805f26e178e/e9ff0/transformer.png 180w,\n/static/7f0926aff76fe3841bfae805f26e178e/f21e7/transformer.png 360w,\n/static/7f0926aff76fe3841bfae805f26e178e/37523/transformer.png 720w,\n/static/7f0926aff76fe3841bfae805f26e178e/302a4/transformer.png 1080w,\n/static/7f0926aff76fe3841bfae805f26e178e/07a9c/transformer.png 1440w,\n/static/7f0926aff76fe3841bfae805f26e178e/807a0/transformer.png 1652w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 53.333333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB20lEQVQoz3WSDW/SUBSG+f+/whgTo2ImiVu24ES+BzIobLaITAcGyiila3tboKV9TC8OOiMnOTm5ze3T97x9M3EcE0Vb/MCl3+/ztdlkMPjO3fAHSldB0zQ09ZtsRenQ6XSYTCYcq0z6sN1uZXuez8IWWK6PaQumps3S9SGOEJ6H4zgIIXBdF9/38TxPnsMwJLNcLtF1nWRGUSTB/mpFd/CLhnLL8LdOfzTFTICAKwTz+ZzFYoFtP+I4Nq5wmc1mEpqZ6lNG47F8kHzZXwXEMegPBqVaHcd19xsk9qRrppt0O33Go+n/V06X8XDPrXKOcIxnwKdOVOU/lTk7LVKrtqVVEpi+lNRmvUHVflKuX/P2wyml+g03vTvm8+UzrxtKk5P8CW31Wq5+ALKDRdEOqE8NcrkWr7ItXryu8vJ9m3fZJuViaw8Mg4Bu8Yp67pJesYFhGHv//658UCiER+FzicpVh9x5gXpTIZ8vsFjsVn96cTYZUiu+Ya7fY1n2QeG/hlvWksvCJZVqmXKlRK1e5SJ/sQfGcSQFaKrKx1wWTdWwrMfjwESBaZpsNoGcYbiVEUnfSabS1ih9adNT+lLEUWAyk7DuAu7JmcQp/ZdlVn2PbRQQhBvW6/Xewz//vz15uMwUIwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"multi-head-attention.png\"\n        title=\"multi-head-attention.png\"\n        src=\"/static/c9494b388aca1b532048a126faa5d346/37523/multi-head-attention.png\"\n        srcset=\"/static/c9494b388aca1b532048a126faa5d346/e9ff0/multi-head-attention.png 180w,\n/static/c9494b388aca1b532048a126faa5d346/f21e7/multi-head-attention.png 360w,\n/static/c9494b388aca1b532048a126faa5d346/37523/multi-head-attention.png 720w,\n/static/c9494b388aca1b532048a126faa5d346/302a4/multi-head-attention.png 1080w,\n/static/c9494b388aca1b532048a126faa5d346/07a9c/multi-head-attention.png 1440w,\n/static/c9494b388aca1b532048a126faa5d346/10ab7/multi-head-attention.png 1552w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>Masked Multi-head attention (핑크색 블럭): 인코더에서는 사용되지 않음</p>\n<hr>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 48.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAACE0lEQVQoz3WSWW/aQBSF/f9/Q6U2ykPTLU33tLSBLEDAGPAGZvMGtilhESRKAYO/aiZKntorje5I98ydc46OslgsWK5WrFYrlsslt7e33N/f82e9Zr3ZsNlu2W63pGnKNk0RlSQJ5XKR87P35E5f8fP7EZpalDPFcRxs22Y4HDKKInr9Pv+rLMtkH41G1DSN43yeFyfvOPzyiV+Xlw8LF9MFrXYb0zQZDAZYts10Ov3nsv1+L+9hGFJr6Jy1E04qDl/1kGLdhmyHIoCbdI3runieJ/t4PGYyuZE9jmNpgajdbsdsNpM4x2njuT2a9Ro3k4Qw8OVcuVve0et36Xa79Ho93MGATseh0+1gWRbC42yfSYaii4rjhGZD48PnQ47ePOP44wHnV/kHyek2ZTa/wfcf2Pm+j2mY6LqBZdms1+sn2Y+ShYdXFZXq2UuKpwdc/3hO/jxHmoHyCBaM2u02rVYLwzAlY9f15Afz+VzKefIwCFDrTa4ucrw9fs1p7htaXZMqlI2IxmYj5Wn1Oo1GA1VVMUwDkYCusMHzpJe7dAcZRFEk8S2nT1VrMozGMiVCjSI8ms6mVKsVamqVcqlEo1HHNAxMy5KLkiiSMsUjccIglOyj0Ygkjgl8jyAImPyeoIjfwqHPRUWlcF2jUKpQ1nQKJZV88ZrvhUtyF0V0Xce2LRmrpt6UORRRa9ktOROMBwOXv+pc1buQe7KRAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"encoder2.png\"\n        title=\"encoder2.png\"\n        src=\"/static/ff7e1c4ff88ab2a2731e3c1a4cd16323/37523/encoder2.png\"\n        srcset=\"/static/ff7e1c4ff88ab2a2731e3c1a4cd16323/e9ff0/encoder2.png 180w,\n/static/ff7e1c4ff88ab2a2731e3c1a4cd16323/f21e7/encoder2.png 360w,\n/static/ff7e1c4ff88ab2a2731e3c1a4cd16323/37523/encoder2.png 720w,\n/static/ff7e1c4ff88ab2a2731e3c1a4cd16323/302a4/encoder2.png 1080w,\n/static/ff7e1c4ff88ab2a2731e3c1a4cd16323/b1001/encoder2.png 1380w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>Position-wise feed-forward network\n<ul>\n<li>2개 이상의 fully connected layer (혹은 dense layer)로 구성</li>\n<li>Token마다 (즉, position 마다) 독립적으로 적용</li>\n<li>첫번째 layer에 ReLU 활성화 함수 사용</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p>![addnormlayer.png](addnor  mlayer.png)</p>\n<hr>\n<h4 id=\"위치정보-임베딩-positional-embedding\" style=\"position:relative;\"><a href=\"#%EC%9C%84%EC%B9%98%EC%A0%95%EB%B3%B4-%EC%9E%84%EB%B2%A0%EB%94%A9-positional-embedding\" aria-label=\"위치정보 임베딩 positional embedding permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>위치정보 임베딩 (Positional embedding)</h4>\n<ul>\n<li>Transformer 모형에서는 단어들의 embedding 정보만을 사용하는 것이 아니라 (단어들이 갖는) 입력된 시퀀스 데이터 내에서의 위치 정보도 사용</li>\n<li>위치 정보를 사용하게 되면, 단어들 간의 위치를 파악함으로써 단어들 간의 상대적인 거리를 파악</li>\n<li>이를 사용하는 주된 이유는, Transformer는 RNN이나 LSTM와 같은 순환신경만 구조(단어들이 순서대로 입력)를 사용하지 않고 attention 방법을 사용하기 때문</li>\n<li>단어들이 갖는 (상대적인) 위치정보를 반영하기 위해서 위치 정보를 반영하는 벡터를 사용하는데 이를 positionla embedding 벡터</li>\n</ul>\n<h4 id=\"masked-self-attention\" style=\"position:relative;\"><a href=\"#masked-self-attention\" aria-label=\"masked self attention permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Masked self-attention</h4>\n<ul>\n<li>Encoder의 self-attention과 약간 다르게 작동 -> 이해하기 위해서는 학습의 단계에서 transformer의 decoder 부분이 어떻게 작동하는지를 먼저 알아야 함</li>\n<li>Teacher forcing 방법 사용\n<ul>\n<li>decoder는 언어모형의 역할을 하지만, 학습의 단계에서는 모형이 현재 단계까지 예측한 단어들의 정보를 사용하여 다음 단어를 예측하는 것이 아니라, 정답 데이터 정보를 이용해서 각 단계의 단어들을 예측하는 것</li>\n<li>모형이 예측한 단어들의 정보를 이용해서 다음 단어를 예측하는 경우에는 이전 단어들에 대한 예측이 잘못되었다면 그 다음 단어에 대한 예측이 제대로 될수 없기 때문</li>\n<li>예 : ‘오늘은 금요일 입니다’을 ‘Today is Friday’로 번역하는 경우\n<ul>\n<li>첫 단어를 ‘Today’라고 예측하지 못하면, 그 다음 단어인 ‘is’를 제대로 예측할 수 없다</li>\n<li>이를 위해 학습에서는 실제 정답 데이터를 사용하여 학습한다.</li>\n<li>앞에 <SOS> 토큰을 더해서 사용 => 논문에서는 right shifted output이라고 표현</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"transformer-응용\" style=\"position:relative;\"><a href=\"#transformer-%EC%9D%91%EC%9A%A9\" aria-label=\"transformer 응용 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Transformer 응용</h4>\n<ul>\n<li>대표적 3가지\n<ul>\n<li>BERT (Bidirectional Encoder Representations from Transformers)\n<ul>\n<li>Transformer의 encoder 부분만을 사용</li>\n<li>주요 사용 용도\n<ul>\n<li>문서/ 단어 임베딩, 문서 분류, Q&#x26;A, 단어들 간의 관계 추출</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>GPT (Generative Pre-trained Transformer)\n<ul>\n<li>Transformer의 decoder 부분만을 사용</li>\n<li>주요 사용 용도\n<ul>\n<li>텍스트 생성</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>BART (Bidirectional and Auto-Regressive Transformers)\n<ul>\n<li>Transformer의 encoder와 decoder 모두 사용</li>\n<li>주요 사용 용도\n<ul>\n<li>텍스트 요약 등</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"bert\" style=\"position:relative;\"><a href=\"#bert\" aria-label=\"bert permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>BERT</h2>\n<h3 id=\"bert의-주-목적\" style=\"position:relative;\"><a href=\"#bert%EC%9D%98-%EC%A3%BC-%EB%AA%A9%EC%A0%81\" aria-label=\"bert의 주 목적 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>BERT의 주 목적</h3>\n<ul>\n<li>Text 분석에서의 전이 학습 (transfer learning)</li>\n<li>학습 데이터\n<ul>\n<li>BooksCorpus (800M words), English Wikipedia (2,500M words)</li>\n</ul>\n</li>\n<li>eㅏㄴ어와 문장의 임베딩 정보를 포함한 모형의 파라미터를 학습 -> downstream tast에 따라서 fine tuning 혹은 feature-based 방법으로 사용 가능</li>\n</ul>\n<h3 id=\"bert의-구조\" style=\"position:relative;\"><a href=\"#bert%EC%9D%98-%EA%B5%AC%EC%A1%B0\" aria-label=\"bert의 구조 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>BERT의 구조</h3>\n<ul>\n<li>Transfomer의 encoder 부분만 사용</li>\n<li>2가지 형태\n<ul>\n<li>BERTBASE (L=12, H=768, A=12, Total Parameters=110M)</li>\n<li>BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)</li>\n<li>L = encoder block의 수, H = 임베딩 벡터 또는 Hidden state 벡터의 차원의 수, A = multi-head attention에서 사용된 attentions의 수</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 52.77777777777778%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACSElEQVQoz4WQ2W7aYBCF/fy9a6s+QG9yE1SlVRrCkgBescEr4IAxhCVglhBACksW/FU4aaVeVB3p02jmH5355whp/YR+FOI6HqZtEXY6zGYzNtst+6cnNtsd7TCk3mhQq9UYDu+YTKZMp1MOhwNxHP+FwHscBxb3c572e3a7HcTvDzG8vr7w/Pyc8L8QWp2IzmBG725Jb7TidrSgP36gP75P8nCyIJotk+HHx0eiKEo4XrFYLJhMJ4yjMePxOKmF81IJyTERvRWy2aSgieSkAiVLR/WXFKouslknjmE+n9M4nl6vJfi+j+e5+Df+m+DDA4Lmr7HDDV59ge1GGNUepj3Cq01xwi1muMW+mRIfYpbLJQ2/QbfbTcSOHBcc/R0MBomngtyMqQQxrhVRsO45Ofe4stY49S2Gt8K4Bau14vDywmq5ot6o0263CYKAZqtJ0G4zGA7Z7fZvHsryBbaVRzZHVOUSnnSGK37HVC9QnB6aUUbVLdbrDd1uB03TqFartI6CzWbyQ9fzCIJ2coGQOfuAmv/Iz1KD/PU3xOwnCpefKWW/kC1KXGW/ksv94O5uThCEVCoVbMeh1+sl9Pv9hDAMEx+FYrpAMV3EcTw0UeNYi5liks2KhXIlUZJ0NNenbN9g2w62Y2MYBrquY+gGpmXiOE5ihaBk06jZNBXDQLnOo2TSqLnLBF1VUHIZxJJIVqkglSs4rovrunie9wf3vWfbNoJaLpPN5zk9TZHL51E0jd+9VCpFJpdDVVVUWUKRJURR/CeSJPELbZAH3e81GWUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"bert.png\"\n        title=\"bert.png\"\n        src=\"/static/44db826d683935715451ecfe83bfd143/37523/bert.png\"\n        srcset=\"/static/44db826d683935715451ecfe83bfd143/e9ff0/bert.png 180w,\n/static/44db826d683935715451ecfe83bfd143/f21e7/bert.png 360w,\n/static/44db826d683935715451ecfe83bfd143/37523/bert.png 720w,\n/static/44db826d683935715451ecfe83bfd143/54bf4/bert.png 1007w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>새로운 토큰\n<ul>\n<li>[CLS] : 입력 시퀀스 데이터 전체의 정보를 반영하기 위한 토큰\n<ul>\n<li>이 토큰에 대한 hidden state는 보통 분류 등의 목적에 사용</li>\n</ul>\n</li>\n<li>[SEP] : 두개의 문장을 구분하기 위한 목적</li>\n</ul>\n</li>\n</ul>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<ul>\n<li>\n<ul>\n<li><a href=\"#activation-function\">Activation function</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#optimizer\">Optimizer</a></p>\n</li>\n<li>\n<p><a href=\"#%EA%B0%80%EC%A4%91%EC%B9%98-%EC%B4%88%EA%B8%B0%ED%99%94\">가중치 초기화</a></p>\n</li>\n<li>\n<p><a href=\"#one-stage-detectors\">One stage detectors</a></p>\n</li>\n<li>\n<p><a href=\"#two-stage-detectors\">TWO stage detectors</a></p>\n</li>\n<li>\n<p><a href=\"#ssd\">SSD</a></p>\n</li>\n<li>\n<p><a href=\"#r-cnn-family\">R-CNN family</a></p>\n</li>\n<li>\n<p><a href=\"#rnn-recurrent-neural-networks\">RNN (Recurrent Neural Networks)</a></p>\n</li>\n<li>\n<p><a href=\"#lstm\">LSTM</a></p>\n</li>\n<li>\n<p><a href=\"#bidirectional-lstm\">Bidirectional LSTM</a></p>\n</li>\n<li>\n<p><a href=\"#gru\">GRU</a></p>\n</li>\n<li>\n<p><a href=\"#seq2seq\">seq2seq</a></p>\n</li>\n<li>\n<p><a href=\"#transformer\">Transformer</a></p>\n<ul>\n<li>\n<p><a href=\"#attention\">Attention</a></p>\n<ul>\n<li><a href=\"#encoder-decoder-attention\">Encoder-decoder attention</a></li>\n<li><a href=\"#self-attention\">Self-attention</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#transformer-1\">Transformer</a></p>\n<ul>\n<li><a href=\"#%EC%9C%84%EC%B9%98%EC%A0%95%EB%B3%B4-%EC%9E%84%EB%B2%A0%EB%94%A9-positional-embedding\">위치정보 임베딩 (Positional embedding)</a></li>\n<li><a href=\"#masked-self-attention\">Masked self-attention</a></li>\n<li><a href=\"#transformer-%EC%9D%91%EC%9A%A9\">Transformer 응용</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#bert\">BERT</a></p>\n<ul>\n<li><a href=\"#bert%EC%9D%98-%EC%A3%BC-%EB%AA%A9%EC%A0%81\">BERT의 주 목적</a></li>\n<li><a href=\"#bert%EC%9D%98-%EA%B5%AC%EC%A1%B0\">BERT의 구조</a></li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"July 22, 2023","title":"Deep learning","categories":"DL","author":"DolmaengC","emoji":"🧢"},"fields":{"slug":"/Deep-learning/"}},"prev":{"id":"fb46d56b-d700-5a53-895c-233ea069b00c","html":"<h1 id=\"treegen-a-tree-based-transformer-architecture-for-code-generation\" style=\"position:relative;\"><a href=\"#treegen-a-tree-based-transformer-architecture-for-code-generation\" aria-label=\"treegen a tree based transformer architecture for code generation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TreeGen: A Tree-Based Transformer Architecture for Code Generation</h1>\n<p>**Zeyu Sun† **</p>\n<p>**Qihao Zhu† **</p>\n<p>**Yingfei Xiong∗† **</p>\n<p>**Yican Sun† Lili Mou‡ **</p>\n<p><strong>Lu Zhang†</strong></p>\n<h2 id=\"abstrct\" style=\"position:relative;\"><a href=\"#abstrct\" aria-label=\"abstrct permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ABSTRCT</h2>\n<p>코드 생성 시스템은 자연어 묘사를 인풋으로 프로그래밍 언어 코드를 생성한다.</p>\n<p>최신 기술들은 뉴럴 네트워크에 의존하는데 , 2가지 문제점이 있다.</p>\n<ol>\n<li>긴 의존성 문제\n<ul>\n<li>코드는 종종 멀리있는 코드 요소의 영향을 받는다.</li>\n</ul>\n</li>\n<li>모델링 구조\n<ul>\n<li>많은 구조적 정보를 가지고 있다.</li>\n</ul>\n</li>\n</ol>\n<p>TreeGen : 새로운 트리 기반 뉴럴 구조 (트렌스포머의 어텐션 메커니즘을 사용)</p>\n<p>벤치마크</p>\n<ul>\n<li>파이썬 벤치마크 : HearthStone</li>\n<li>semantic parsing 벤치마크 : ATIS, GEO</li>\n</ul>\n<h1 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>INTRODUCTION</h1>\n<p>코드 생성은 개발자들의 생산성을 향상시키기 위한 중요한 인공지능이다.</p>\n<p>Seq2Seq, Seq2Tree 모델 등 다양한 신경망 구조를 갖는다.</p>\n<p>최신 접근법은 문법 규칙의 sequence를 예측하는 코드를 생성하는 것이다.</p>\n<p>이 논문은 새로운 신경망 구조 (TreeGen)을 제안한다.</p>\n<p>TreeGen은 트렌스포머 구조를 제안하지만, 기존의 트렌스포머는 프로그램용으로 설계되어 있지 않았으며,</p>\n<p>트리 구조에 최적화 되어 있지 않다.</p>\n<p>최적화를 위해서는 그래프, 트리 기반의 컨볼루션 신경망 구조를 가져야 한다.</p>\n<p>TreeGen은 3부분으로 구성한다.</p>\n<ol>\n<li>\n<p>A natural language reader (encoder) : encode the text description.</p>\n</li>\n<li>\n<p>A AST reader (the first several transformer decoder blocks) :</p>\n<p>encode the previously generated partial code with the structural convolution sub-layers</p>\n</li>\n<li>\n<p>A decoder (the rest transformer decoder blocks) :</p>\n<p>combine the query and previous two encoders to predict the next grammer rule.</p>\n</li>\n</ol>\n<h2 id=\"treegen\" style=\"position:relative;\"><a href=\"#treegen\" aria-label=\"treegen permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TreeGen</h2>\n<p>![스크린샷 2023-08-01 오후 2.22.20](/Users/choejunhyeog/Desktop/스크린샷 2023-08-01 오후 2.22.20.png)</p>\n<h3 id=\"1-grammar-rule-prediction\" style=\"position:relative;\"><a href=\"#1-grammar-rule-prediction\" aria-label=\"1 grammar rule prediction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Grammar Rule Prediction</h3>\n<p>decomposed into several context-free grammar rules and parsed as an AST</p>\n<p>AST-based code generation could be thought of as expanding a non-terminal node by a grammar rule. This process is repeated until all leaf nodes are terminal.</p>\n<p>선주문 트래버스에 따라 오른쪽 상단 모서리에 표시된 AST를 생성하는 일련의 규칙을 얻을 수 있습니다.</p>\n<p>Formally, the probability can be factorized as the proba- bilities of the rules generating the code following the order.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 9.444444444444445%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAZ0lEQVQI101M2wpAERD0/9/mCzyRyCUKJSRz2n06U9vONBeRUkIIATln7L0hpYQxBucckNdag7WWudaa9VoL7z3MOZnTJ9CO8N7DOceF/2ApBeT13jlIGaUUaq0YY+Deyx4daUKMER+TypbH1ZfKIgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"1.png\"\n        title=\"1.png\"\n        src=\"/static/f55283156537a0436bd2e7ecaa41f66d/37523/1.png\"\n        srcset=\"/static/f55283156537a0436bd2e7ecaa41f66d/e9ff0/1.png 180w,\n/static/f55283156537a0436bd2e7ecaa41f66d/f21e7/1.png 360w,\n/static/f55283156537a0436bd2e7ecaa41f66d/37523/1.png 720w,\n/static/f55283156537a0436bd2e7ecaa41f66d/302a4/1.png 1080w,\n/static/f55283156537a0436bd2e7ecaa41f66d/5f652/1.png 1302w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>our task is to train a model to calculate p(ri | NL input, pi )</p>\n<h3 id=\"2-nl-reader\" style=\"position:relative;\"><a href=\"#2-nl-reader\" aria-label=\"2 nl reader permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. NL Reader</h3>\n<p>The input description determines the functionality of the code.</p>\n<ol>\n<li>Tokenize input description into tokens.</li>\n<li>All the tokens and characters are represented as real-valued vectors n1, n2,…,nL and c1, c2, … , cs by embeddings.</li>\n</ol>\n<p>In summary, the NL reader has a few Transformer blocks</p>\n<p>of self-attention, the gating mechanism, and word convolu-</p>\n<p>tion. The natural language description is encoded as features y(NL), y(NL), · · · , y(NL).</p>\n<h3 id=\"3-input-text-representation\" style=\"position:relative;\"><a href=\"#3-input-text-representation\" aria-label=\"3 input text representation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Input Text Representation</h3>\n<h4 id=\"character-embedding\" style=\"position:relative;\"><a href=\"#character-embedding\" aria-label=\"character embedding permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Character Embedding</h4>\n<p>Similar words have similar characters (e.g. “program” and “programs”)</p>\n<p>a token by character embeddings with <strong>a fully-connected layer</strong></p>\n<p>![스크린샷 2023-08-01 오후 5.37.43](/Users/choejunhyeog/Desktop/스크린샷 2023-08-01 오후 5.37.43.png)</p>\n<p>W(c) are the weights and the character sequence is padded to a pre-defined maximum length M.</p>\n<p><strong>Layer normalization</strong></p>\n<p>이 벡터들은 NL reader로 넘어가고 <strong>gating sub-layer</strong>에서 워드 임베딩과 통합된다.</p>\n<h3 id=\"4-neural-structure-of-nl-reader\" style=\"position:relative;\"><a href=\"#4-neural-structure-of-nl-reader\" aria-label=\"4 neural structure of nl reader permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. Neural Structure of NL Reader</h3>\n<p>The NL reader is composed of a stack of blocks.</p>\n<p>Each block contains three different sub-layers (self-attention, gating mechanism, word convolution)</p>\n<h4 id=\"self-attention\" style=\"position:relative;\"><a href=\"#self-attention\" aria-label=\"self attention permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Self-Attention</h4>\n<p>Transformer’s architecture</p>\n<p>Multi-head attention (to capture long dependency information)</p>\n<ol>\n<li>Embedding by a look-up table</li>\n<li>position embeddings</li>\n<li>Variant (Dehghani et al.)[<a href=\"https://arxiv.org/abs/1807.03819\">https://arxiv.org/abs/1807.03819</a>]</li>\n<li>Compute the position embedding for the 4th word in the word in the bth Transformer block as</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 16.666666666666664%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAl0lEQVQI1z2P0QqFIBBE/f/vK0GsyKxIrSyoh6KY2OFyhUV0Zs/OKvzOuq7w3qOqKuz7jhACrLWIMVJzzqHrOpRliaIoqIv/vm/255zRti3U+76QmqYJKSUCzvMktO973sdxECCaQAW+bRv/nuchUDyiKyE3TUPTPM+ctCwLxnGEMYYmect0SSQJtdZMPQwDruv6b1jXNT7N5eCnGOZARQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"34.png\"\n        title=\"34.png\"\n        src=\"/static/bd49c69e3c7fa5ba9ee0bff0e21393db/37523/34.png\"\n        srcset=\"/static/bd49c69e3c7fa5ba9ee0bff0e21393db/e9ff0/34.png 180w,\n/static/bd49c69e3c7fa5ba9ee0bff0e21393db/f21e7/34.png 360w,\n/static/bd49c69e3c7fa5ba9ee0bff0e21393db/37523/34.png 720w,\n/static/bd49c69e3c7fa5ba9ee0bff0e21393db/302a4/34.png 1080w,\n/static/bd49c69e3c7fa5ba9ee0bff0e21393db/1132d/34.png 1158w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h4 id=\"gating-mechanism\" style=\"position:relative;\"><a href=\"#gating-mechanism\" aria-label=\"gating mechanism permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Gating Mechanism</h4>\n<p>Self-attention에 의해 특징들이 계산된 후에, character 임베딩 정보와 통합한다.</p>\n<p>gating mechanism은 softmax를 베이스로 한다.</p>\n<h4 id=\"word-convolution\" style=\"position:relative;\"><a href=\"#word-convolution\" aria-label=\"word convolution permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Word Convolution</h4>\n<p>2개의 컨볼루션 레이어가 gating 메카니즘의 아웃풋에 적용된다.</p>\n<p>여기서 seperable convolution이 사용된다.</p>\n<ul>\n<li>seperable convolution은 파라미터가 적어서 학습이 쉽다.</li>\n</ul>\n<p>첫번째와 마지막 단어를 위해 zero padding을 추가한다.</p>\n<p>GELU 활성화 함수 사용</p>\n<h3 id=\"5-ast-reader\" style=\"position:relative;\"><a href=\"#5-ast-reader\" aria-label=\"5 ast reader permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. AST Reader</h3>\n<p>생성된 부분적 AST의 구조를 모델링하기 위해 AST reader를 만들었다.</p>\n<p>문법 규칙의 순서를 예측하여 생성하지만, 이 규칙들만으로는 프로그램에 대한 구체적인 그림이 부족하고 다음 규칙을 예측하기에는 불충분하다.</p>\n<p>-> AST Reader가 예측된 정보와 트리구조를 포함한 heterogeneous 정보를 고려한다.</p>\n<p>​\t이러한 프로그램별 정보를 통합하기 위해 먼저 코드를 일련의 규칙으로 표현한 다음 어텐션 메커니즘으로 규칙을 인코딩하고 마지막으로 트리 컨볼루션 레이어를 사용하여 각 노드의 인코딩된 표현을 조상과 결합합니다.</p>\n<h3 id=\"6-ast-representation\" style=\"position:relative;\"><a href=\"#6-ast-representation\" aria-label=\"6 ast representation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>6. AST Representation</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 27.77777777777778%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABYlAAAWJQFJUiTwAAAA/0lEQVQY01VQ6YqGMAz0/V9tBVlR8frhjda23gjqfrNMwIUVxjRpMpOpE4YhCKUU5nnGNE2w1sq573tkWYZ1XVFVFYIgQFmW2LZNasuy/Js5jgMOGzjUti26rvvDOI5CmOe5xKIo4Pu+IE1TRFEk4CLDMKBpGhFw3kGCF2/k9zyPkHML1pIkwevI8zzEcSxnCvCefQ6Tt4m2SEg1Wtj3XTanJda4HYlc9wtZlgp8/xuu64rwfd9wjDHQWos6Fd8NCdphzljXtdhPkhxxnGEYNJSyGEcLpbTYva4LDn8kZYH2CebckCAZBY0hZhhzwtoTxhxQaofWO7btxOfzI0/0CwXHu1deAv0BAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"production-rule\"\n        title=\"production-rule\"\n        src=\"/static/e7b5cc0bc11ac5d28122b7c329270c0d/37523/production-rule.png\"\n        srcset=\"/static/e7b5cc0bc11ac5d28122b7c329270c0d/e9ff0/production-rule.png 180w,\n/static/e7b5cc0bc11ac5d28122b7c329270c0d/f21e7/production-rule.png 360w,\n/static/e7b5cc0bc11ac5d28122b7c329270c0d/37523/production-rule.png 720w,\n/static/e7b5cc0bc11ac5d28122b7c329270c0d/302a4/production-rule.png 1080w,\n/static/e7b5cc0bc11ac5d28122b7c329270c0d/07a9c/production-rule.png 1440w,\n/static/e7b5cc0bc11ac5d28122b7c329270c0d/c679a/production-rule.png 2460w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p><strong>Rule Sequence Embedding</strong></p>\n<ul>\n<li>규칙 정보를 인코딩하기 위해서 규칙 ID를 사용한다.</li>\n<li>규칙들을 table-lookup 임베딩으로 real-valued vectors로 나타낸다.</li>\n</ul>\n<p><strong>Rule Definition Encoding</strong></p>\n<ul>\n<li>위의 테이블 조회 임베딩은 문법 규칙을 원자 토큰으로 취급하고 규칙 내용의 정보를 잃습니다.</li>\n<li>이 문제를 완화하기 위해 규칙 정의의 인코딩으로 규칙 표현을 향상시킵니다.</li>\n</ul>\n<p><strong>Position and Depth Embeddings</strong></p>\n<p>Position embedding</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 8.333333333333332%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAcUlEQVQI11WOuwoFIRBD9/+/zkIr8Y2yCiKKiIWYixYXdqpDksnM473HGAMpJbTWUGuFtRbOucu9dxhjEGOEUgqEEDDGIKX863tvnFlr4TniWXzf9xbmnME5hxDicikFWmucwydLKb3enPM+EkL4FP4AKMyVjwqm6GgAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"4.png\"\n        title=\"4.png\"\n        src=\"/static/0597f682568693ed51f875d060c15793/37523/4.png\"\n        srcset=\"/static/0597f682568693ed51f875d060c15793/e9ff0/4.png 180w,\n/static/0597f682568693ed51f875d060c15793/f21e7/4.png 360w,\n/static/0597f682568693ed51f875d060c15793/37523/4.png 720w,\n/static/0597f682568693ed51f875d060c15793/302a4/4.png 1080w,\n/static/0597f682568693ed51f875d060c15793/b490f/4.png 1142w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3 id=\"7-neural-structure-of-ast-reader\" style=\"position:relative;\"><a href=\"#7-neural-structure-of-ast-reader\" aria-label=\"7 neural structure of ast reader permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>7. Neural Structure of AST Reader</h3>\n<p>four sub-layers (self-attention, gating mechanism, NL attention, tree convolution)</p>\n<p>residual connection except the layer of tree convolution</p>\n<p>layer normalization</p>\n<p><strong>Self-Attention</strong></p>\n<p>Transformer-like self-attention layer</p>\n<p>input : sum of the rule embedding, position embedding, dept embedding</p>\n<p>extract features</p>\n<p><strong>Gating Mechanism</strong></p>\n<p>Content-encoding rule representation</p>\n<p>Transformer-extracted features</p>\n<p><strong>NL Attention</strong></p>\n<p>Multi-head NL attention (similar to the Transformer decoder’s attention to its encoder)</p>\n<p><strong>Tree Convolution</strong></p>\n<h3 id=\"8-decoder\" style=\"position:relative;\"><a href=\"#8-decoder\" aria-label=\"8 decoder permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>8. Decoder</h3>\n<h3 id=\"9-training-and-inference\" style=\"position:relative;\"><a href=\"#9-training-and-inference\" aria-label=\"9 training and inference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>9. Training and Inference</h3>","frontmatter":{"date":"August 02, 2023","title":"TreeGen","categories":"DL","author":"DolmaengC","emoji":"🧢"},"fields":{"slug":"/TreeGen/"}},"site":{"siteMetadata":{"siteUrl":"https://dolmaengc.github.io/gatsby-blog","comments":{"utterances":{"repo":"DolmaengC/gatsby-blog"}}}}},"pageContext":{"slug":"/index-in-blog-error/","nextSlug":"/Deep-learning/","prevSlug":"/TreeGen/"}},"staticQueryHashes":["1073350324","1956554647","2938748437"]}