{"componentChunkName":"component---src-templates-blog-template-js","path":"/Deep-learning/","result":{"data":{"cur":{"id":"d753440e-354a-5233-ad8b-5464f524dd69","html":"<p>Summary about Deep learning lecture</p>\n<p><em>Originally from <a href=\"https://www.youtube.com/playlist?list=PLAudaEp1AjCF3EPZ8ZP8R3qy0iABN8uNN\">Youtube lecture</a></em></p>\n<hr>\n<h4 id=\"activation-function\" style=\"position:relative;\"><a href=\"#activation-function\" aria-label=\"activation function permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Activation function</h4>\n<p>주요 활성화 함수</p>\n<ul>\n<li>logistic 함수 (Sigmoid function) : 요즘은 잘 안씀</li>\n<li>Hyperblolic Tangent (tanh) : 텍스트, sequence data, 다른 분야에선 ㄴㄴ</li>\n<li>Rectified Linear Unit (Relu) : 이미지, 주로 많이 사용됨</li>\n<li>Leaky Relu</li>\n<li>Exponential linear unit(ELU)</li>\n</ul>\n<p>경사손실 문제 : 은닉층의 한 활성함수값이 0에 가까워지면 다른 은닉층의 값에 상관없이 0에 가까워지는 문제 -> 은닉층이 많을 수록 발생할 확률이 커져서 많이 쌓기 어렵다 -> Relu가 많이 사용되는 이유</p>\n<h2 id=\"optimizer\" style=\"position:relative;\"><a href=\"#optimizer\" aria-label=\"optimizer permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Optimizer</h2>\n<p>주로 Adam, RMSprop, Adadelta가 사용됨</p>\n<ul>\n<li>\n<p>Momentum : 이전 업데이트 정보를 기억, 현재 업데이트에 반영하는 방법</p>\n<ul>\n<li>장점 (GD에 비해서)\n<ul>\n<li>local minimum을 잘 피한다.</li>\n<li>수렴하는 속도가 빠르다.</li>\n</ul>\n</li>\n<li>주로 다른 기법과 같이 사용됨</li>\n</ul>\n</li>\n<li>\n<p>Adagrad (Adaptive Gradient)</p>\n<ul>\n<li>GD -> 업데이트 횟수와 상관없이 learning rate를 동일하게</li>\n<li>하지만, Adagrad는 다르게\n<ul>\n<li>지금까지 업데이트 된 정도를 반영한다.</li>\n<li>지금까지 업데이트가 많이된 파라미터는 learning rate를 작게</li>\n</ul>\n</li>\n<li>주요 문제\n<ul>\n<li>GT가 갈수록 커진다. -> 업데이트가 거의 발생하지 않는다. -> 최소지점에 도착하기 전에 업데이트가 멈출 수 있다.</li>\n<li>Adadelta\n<ul>\n<li>무조건적으로 줄어드는 learning rate 문제를 보완</li>\n<li></li>\n</ul>\n</li>\n<li>RMSprop\n<ul>\n<li>무조건적으로 줄어드는 learning rate문제를 보완</li>\n<li>합이 아니라 평균을 사용</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Adam</p>\n<ul>\n<li>RMSprop (or Adadelta) + momentum</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"가중치-초기화\" style=\"position:relative;\"><a href=\"#%EA%B0%80%EC%A4%91%EC%B9%98-%EC%B4%88%EA%B8%B0%ED%99%94\" aria-label=\"가중치 초기화 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>가중치 초기화</h2>\n<p>Random Value로 초기화</p>\n<ul>\n<li>\n<p>활성화 함수 : Sigmoid or tanh 인 경우</p>\n<ul>\n<li>\n<p>Xavier Weight Initialization : uniform 분포하는  기법</p>\n<ul>\n<li>\n<p>일반적으로 많이 쓰임</p>\n</li>\n<li>\n<p>Keras에서 기본신경망의 경우, kernel_initilization=‘glory_uniform’로 설정되어 있음</p>\n<p>->Xavier 방법</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Normalized Xavier Weight Initialization : 첫번째랑 비슷한데 구간을 나눔, 잘 안쓰임</p>\n</li>\n</ul>\n</li>\n<li>\n<p>활성화 함수 : relu</p>\n<ul>\n<li>He Weight Initialization\n<ul>\n<li>주로 이미지 분석(CNN 알고리즘)에서 많이 사용됨</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"object-detection\" style=\"position:relative;\"><a href=\"#object-detection\" aria-label=\"object detection permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Object detection</h1>\n<p>1단계 : 물체가 있을 만한 경계상자(regions of interest, ROI) 추출</p>\n<p>2단계 : 추출된 ROI를 이용하여 localization과 classification을 수행</p>\n<h2 id=\"one-stage-detectors\" style=\"position:relative;\"><a href=\"#one-stage-detectors\" aria-label=\"one stage detectors permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>One stage detectors</h2>\n<ul>\n<li>앞의 두 단계를 한번에 수행</li>\n<li>SSD, YOLO 등</li>\n</ul>\n<h2 id=\"two-stage-detectors\" style=\"position:relative;\"><a href=\"#two-stage-detectors\" aria-label=\"two stage detectors permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TWO stage detectors</h2>\n<ul>\n<li>앞의 두 단계를 구분해서 수행</li>\n<li>R-CNN family</li>\n<li>속도가 느리다는 단점</li>\n</ul>\n<h2 id=\"ssd\" style=\"position:relative;\"><a href=\"#ssd\" aria-label=\"ssd permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SSD</h2>\n<p>Feagure map 추출</p>\n<ul>\n<li>Object detection을 위해서 사용하는 feature map의 수 6개\n<ul>\n<li>더 많은 feature map이 되출되지만 그중 특정 6개만 사용</li>\n</ul>\n</li>\n<li>1차적으로 VGG16과 같은 사전학습 모형을 사용하여 feature map 추출, 이후 추가적인 convolutional layer를 사용하여 6개의 feature map 추출\n<ul>\n<li>VGG16dl cncnfgks feature map 중 1개 사용 + 추가 convolutional layer가 추출한 6개중 5개 사용\n<ul>\n<li>Feature map 마다의 크기가 다음</li>\n</ul>\n</li>\n<li>Multi-scale object detection\n<ul>\n<li>크기가 다른 여라개의 feature map을 사용하여 물체 탐지</li>\n<li>다양한 크기의 물체를 잘 찾을 수 있음</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Non-maximum suppression\n<ul>\n<li>확률이 가장 큰 anchor box를 선택</li>\n<li>특정 클래서에 대해서\n<ul>\n<li>확룰이 특정값 (예, 0.1) 이하인 상자는 모두 삭제</li>\n<li>남아있는 상자들에 대해서 확률값이 제일 큰 상자를 선택\n<ul>\n<li>해당 상자와 IoU 값이 0.45 이상인 다른 상자들을 모두 삭제</li>\n</ul>\n</li>\n<li>남아 있는 상자들에 대해서 동일한 과정 반복\n<ul>\n<li>확률값이 제일 큰 상자를 선택 -> 해당 상자와 IoU 값이 0.45 이상인 다른 상자들을 모두 삭제</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"r-cnn-family\" style=\"position:relative;\"><a href=\"#r-cnn-family\" aria-label=\"r cnn family permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>R-CNN family</h2>\n<p>R-CNN in 2014, Fast R-CNN in 2015, Faster R-CNN in 2016</p>\n<ol>\n<li>Input images</li>\n<li>Extract region proposals(~2k)</li>\n<li>Compute CNN features</li>\n<li>Classify regions</li>\n</ol>\n<ul>\n<li>크게 4부분으로 구성\n<ul>\n<li>Extract regions of interest(ROIs)\n<ul>\n<li>Selective search (최근엔 사용하지 않음)</li>\n</ul>\n</li>\n<li>Feature extraction module\n<ul>\n<li>이전 단계에서 추출된 ROI들에 대해 CNN을 적용해서 feature map 추출</li>\n</ul>\n</li>\n<li>Classification module\n<ul>\n<li>이전 단계에서 추출된 feature들에 대해서 classification 예측</li>\n<li>Support vector machine (SVM) 사용</li>\n</ul>\n</li>\n<li>Localization module\n<ul>\n<li>ROI를 기준으로 GTBB에 대한 offset을 예측함</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>R-CNN의 주요 단점\n<ul>\n<li>계산량이 많아 시간이 오래 걸린다.</li>\n</ul>\n</li>\n<li>Fast R-CNN\n<ul>\n<li>사전 학습모형을 먼저 적용하여 feature map 추출-> 그 다음에 selective search 방법 적용하여 ROI 추출</li>\n<li>SVM 대신 fully connected layer 사용</li>\n<li>단점 : 여전히 selective search 방법을 사용해서 속도가 느림</li>\n</ul>\n</li>\n<li>Faster R-CNN\n<ul>\n<li>Region Proposal Network 방식 사용\n<ul>\n<li>VGG16 등의 사전학습모형을 사용하여 feature map 추출</li>\n<li>Feature map의 각 셀에 대해서 크기와 형태가 다른 9개의 anchor box 생성</li>\n<li>각 anchor box에 대해서 두 가지 종류의 값들 예측\n<ul>\n<li>Objectness score : 물체가 있을 확률</li>\n<li>GTBB와의 offset 값들</li>\n</ul>\n</li>\n<li>Anchor box에 예측된 offset을 적용하여 ROIs를 추출</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"rnn-recurrent-neural-networks\" style=\"position:relative;\"><a href=\"#rnn-recurrent-neural-networks\" aria-label=\"rnn recurrent neural networks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RNN (Recurrent Neural Networks)</h2>\n<ul>\n<li>Sequence data를 다루기에 적합</li>\n<li>텍스트 데이터 분석에 적합\n<ul>\n<li>단어들의 문맥적 의미 추출에 용이</li>\n<li>단어들 간의 관계 정보 추출에 용이</li>\n<li>어떠한 단어들이 어떠한 순서로 언제 사용되었는지에 대한 정보 추출 용이</li>\n</ul>\n</li>\n<li>언어모형 (language model), 분류(예, 감성분석), 기계 번역</li>\n</ul>\n<h2 id=\"lstm\" style=\"position:relative;\"><a href=\"#lstm\" aria-label=\"lstm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>LSTM</h2>\n<ul>\n<li>RNN 기반의 신경망 알고리즘</li>\n<li>Simple RNN의 문제를 보완하기 위해 제안\n<ul>\n<li>Problem of long term dependency\n<ul>\n<li>입려괸 문서에서 상대적으로 오래전에 사용된 단어의 정보가 잘 전달 되지 않는다.</li>\n<li>주요 원인 : 경사손실문제</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Forget 게이트\n<ul>\n<li>역할 : 이전 기억 셀 (즉, ct-1)이 가지고 있는 정보 중에서 정답을 맞히는데 불필요한 정보는 잊어버리는 역할\n<ul>\n<li>Ct-1이 가지고 있는 각 원소의 정보 중에서 몇 %를 잊어버릴 것이냐를 결정하기 위해서 0~1사이의 값을 반환하는 sigmoid 함수를 사용</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Input 게이트\n<ul>\n<li>일부의 정보가 삭제된 ct-1(즉, ct-1’)에 새로운 정보를 추가하는 역할</li>\n<li>일단, 추가하고자 하는 정보를 계산 : ht-1와 단어t의 정보를 사용</li>\n<li>새롭게 추가되는 정보들의 긍부정 역할을 나타내기 위해 -1~1의 값을 출력하는 tanh()를 사용</li>\n<li>그대로 반영되는 것이 아니라, 정답을 맞히는데 있어서 기여하는정도에 따라서 적용되는 비율을 다르게 함 -> 이를 위해 sigmoid 함수 사용</li>\n</ul>\n</li>\n<li>Output 게이트\n<ul>\n<li>Output 게이트의 역할은 forget게이트와 input 게이트를 이용하여 업데이트된 기억셀의 정보, 즉 ct,를 이용하여 ht를 계산하는 것</li>\n<li>output 게이트는 ct가 갖는 원소의 값들을 조정하여 ht를 계산</li>\n<li>ct가 갖고 있는 원소들 중에서 정답을 맞히는데 중요한 역할을 하는 원소의 비중은 크게하고, 그렇지 않은 원소들의 비중은 작게해서 ht를 계산</li>\n<li>각 원소의 비중을 계산하기 위해서 현재 LSTM층에 입력되는 ht-1과 단어t의 정보(xt) 인자로 갖는 sigmoid 함수를 사용</li>\n<li>ct원소들의 긍부정 역할을 구분하기 위해 tanh 적용</li>\n</ul>\n</li>\n</ul>\n<img width=\"868\" alt=\"스크린샷 2023-07-21 오전 10 20 06\" src=\"https://github.com/DolmaengC/DolmaengC.github.io/assets/107832431/16a9ca88-8e1e-45fb-82c8-6170ec975bb3\">\n<h2 id=\"bidirectional-lstm\" style=\"position:relative;\"><a href=\"#bidirectional-lstm\" aria-label=\"bidirectional lstm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Bidirectional LSTM</h2>\n<img width=\"890\" alt=\"스크린샷 2023-07-21 오전 10 22 09\" src=\"https://github.com/DolmaengC/DolmaengC.github.io/assets/107832431/466927bc-3d09-41a4-ae18-10c162481736\">\n<h2 id=\"gru\" style=\"position:relative;\"><a href=\"#gru\" aria-label=\"gru permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GRU</h2>\n<ul>\n<li>\n<p>LSTM보다 성능이 떨어져서 잘 사용되지 않음</p>\n</li>\n<li>\n<p>게이트 개념 사용</p>\n<ul>\n<li>\n<p>기억셀을 사용하지 않음</p>\n</li>\n<li>\n<p>Rest 게이트와 update 게이트</p>\n<ul>\n<li>\n<p>hidden state 정보를 업데이트하기 위해서 reset 게이트와 update 게이트 사용</p>\n</li>\n<li>\n<p>GRU는 LSTM 보다는 간단한 구조</p>\n<ul>\n<li>속도는 빠느라 정확도가 떨어짐</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<img width=\"838\" alt=\"스크린샷 2023-07-21 오전 10 09 57\" src=\"https://github.com/DolmaengC/DolmaengC.github.io/assets/107832431/1b06c03f-76f3-4410-b078-c3c334d4cfae\">\n<h2 id=\"seq2seq\" style=\"position:relative;\"><a href=\"#seq2seq\" aria-label=\"seq2seq permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>seq2seq</h2>\n<ul>\n<li>\n<p>주요 목적</p>\n<ul>\n<li>To convert sequences from one domain to sequences in anotjer domain, 번역이 대표적인 예</li>\n<li>Encoder-decoder 구조</li>\n<li>Encoder의 역할\n<ul>\n<li>입력뙨 텍스트 데이터를 숫자 형태로 혹은 벡터 형태로 변환</li>\n</ul>\n</li>\n</ul>\n<img width=\"798\" alt=\"스크린샷 2023-07-21 오전 11 06 12\" src=\"https://github.com/DolmaengC/DolmaengC.github.io/assets/107832431/610972e5-86b6-4a31-bf05-571b920d2c8c\">\n<ul>\n<li>Decoder의 역할\n<ul>\n<li>Encoder에 의해 숫자로 변경된 정보를 다른 형태의 텍스트 데이터로 변환</li>\n</ul>\n</li>\n</ul>\n<img width=\"1261\" alt=\"스크린샷 2023-07-21 오전 11 06 37\" src=\"https://github.com/DolmaengC/DolmaengC.github.io/assets/107832431/af3853a1-3362-4ade-ac83-214813756795\">\n<ul>\n<li>Encoder와 Decoder를 위해 순환신경망 기반 모형 사용 가능 (예, RNN)\n<ul>\n<li>첫번째 RNN이 encoder 역할, 두번째 RNN이 decoder 여할</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"transformer\" style=\"position:relative;\"><a href=\"#transformer\" aria-label=\"transformer permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Transformer</h2>\n<ul>\n<li>소개\n<ul>\n<li>2017년에 google에서 제안한 attention 기반의 encoder-decoder 알고리즘\n<ul>\n<li>순환신경망 기반의 방법이 아니라 attention 사용</li>\n<li>주요 applications:\n<ul>\n<li>BERT (Bidirectional Encoder Representations from Transformers)\n<ul>\n<li>encoder만 사용</li>\n<li>단어 embedding</li>\n<li>문서 embedding</li>\n<li>분류</li>\n<li>Q&#x26;A</li>\n</ul>\n</li>\n<li>GPT (Generative Pre-trained Transformer)\n<ul>\n<li>decoder만 사용</li>\n<li>생성모델</li>\n</ul>\n</li>\n<li>BART (Bidirectional and Auto-Regressive Transformers)\n<ul>\n<li>Encoder, decoder 둘 다 사용</li>\n<li>텍스트 요약</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Transfomer를 이해하기 위해서는 attention을 먼저 이해하는 것이 필요</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"attention\" style=\"position:relative;\"><a href=\"#attention\" aria-label=\"attention permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Attention</h3>\n<h4 id=\"encoder-decoder-attention\" style=\"position:relative;\"><a href=\"#encoder-decoder-attention\" aria-label=\"encoder decoder attention permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Encoder-decoder attention</h4>\n<ul>\n<li>Why was it proposed?\n<ul>\n<li>순환신경망 기반의 seq2seq 모형이 갖는 문제점을 보완하기 위해</li>\n<li>순환신경망 기반의 seq2seq의 주요한 문제점\n<ul>\n<li>입력된 sequence data에 대해서 하나의 고정된 벡터 정보(마지막 hidden state)만을 decoder로 전달한다는 것</li>\n</ul>\n</li>\n<li>그렇다면 어떻게 하면 되는가?\n<ul>\n<li>Encoder 부분에서 생성되는 각 단어에 대한 hidden state 정보를 모두 decoder로 전달</li>\n<li>벡터정보들을 쌓아서 행렬로 만들어서 전달</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<img width=\"1298\" alt=\"스크린샷 2023-07-21 오후 12 47 34\" src=\"https://github.com/DolmaengC/DolmaengC.github.io/assets/107832431/d27533ad-e144-4a43-a557-c29066f9a241\">\n<ul>\n<li>가중치의 계산\n<ul>\n<li>가중치는 hs의 각 hidden state와 decoder에 예측하고자 하는 단어에 대한 hidden state와의 유사도를 가지고 계산</li>\n<li>Hidden state 간의 유사도를 계산 -> 내적 연산</li>\n<li>decoder 부분의 첫번째 RNN층에서 출력되는 ‘Today’ 단어를 예측하는데 사용되는 hidden state -> hd,0\n<ul>\n<li>hd,0 = (1 0 0 0 2)</li>\n</ul>\n</li>\n<li>h0, h1, h2와 hd,0과의 내적 연산\n<ul>\n<li>(1 0 0 1 2) *(1 0 0 0 2) = 1 + 4 = 5</li>\n<li>(1 0 0 1 1) *(1 0 0 0 2) = 1 + 2 = 3</li>\n<li>(1 0 0 0 1) *(1 0 0 0 2) = 1 + 2 = 3</li>\n<li>이 값들을 attention score라고 함\n<ul>\n<li>Attention score의 값이 클수록 관련도가 크다는 것을 의미</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Attention score를 가지고 가중치 계산</li>\n<li>가중치는 확률값으로 표현</li>\n<li>확률값을 계산하기 위해서 attention score에 softmax()를 적용</li>\n</ul>\n</li>\n<li>최종적으로 출력되는 값\n<ul>\n<li>Attention에서 출력되는 값과 RNN 층에서 출력되는 값 간의 이어붙이기(concatenation)</li>\n<li>즉, Concat((1.0 0.1 0 0.9 1.8), (1 0 0 0 2))</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"self-attention\" style=\"position:relative;\"><a href=\"#self-attention\" aria-label=\"self attention permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Self-attention</h4>\n<p>Attention과의 차이</p>\n<ul>\n<li>Attention은 encoder-decoder 모형에서 보통 decoder에서 encoder에서 넘어오는 정보에 가중치를 주는 식으로 작동</li>\n<li>Self-attention은 입력된 텍스트 데이터 내에 존재하는 단어들간의 관계를 파악하기 위해 사용\n<ul>\n<li>관련이 높은 단어에 더 많은 가중치를 주기 위해 사용</li>\n</ul>\n</li>\n<li>지시대명사가 무엇을 의미하는지 등을 파악하는데 유용</li>\n</ul>\n<p>Transformer에서의 self-attention (or attention)</p>\n<ul>\n<li>\n<p>Transformer의 self-attention은 입력 받은 단어들 중에서 어떠한 단어에 더 많은 가중치를 줘야 하는지 파악하기 위해서 각 단어들에 대한 Query, Key, Value 라고 하는 서로 다른 3개의 벡터들을 사용</p>\n<ul>\n<li>Key, Value 벡터들은 사전 형태의 데이터 의미 : Key는 단어의 id와 같은 역할, Value는 해당 단어에 대한 구체적 정보를 저장하는 역할</li>\n<li>Query 벡터는 윳한 다른 단어를 찾을 때 사용되는 (질의) 벡터라고 생각 가능</li>\n</ul>\n</li>\n<li>\n<p>작동 순서</p>\n<ul>\n<li>단계1 : 입력된 각 단어들에 대해서 Query, Key, Value 벡터를 계산\n<ul>\n<li>이떄 각각의 가중치 행렬이 사용됨</li>\n</ul>\n</li>\n</ul>\n<img width=\"755\" alt=\"스크린샷 2023-07-21 오후 1 20 33\" src=\"https://github.com/DolmaengC/DolmaengC.github.io/assets/107832431/d5d77efa-74d0-48ff-8dd1-6f1380076e8a\">\n<img width=\"650\" alt=\"스크린샷 2023-07-21 오후 1 21 40\" src=\"https://github.com/DolmaengC/DolmaengC.github.io/assets/107832431/108fbabf-ef05-4249-82e6-90873b69710e\">\n<ul>\n<li>\n<p>단계2 : Attention score 계산</p>\n<ul>\n<li>Query를 이용하여 각 Key들하고의 유사한 정도를 계산 -> 내적 연산</li>\n</ul>\n</li>\n<li>\n<p>단계3 : Attention score를 이용하여 가중치 계산</p>\n<ul>\n<li>Softmax() 함수를 적용</li>\n</ul>\n</li>\n</ul>\n<img width=\"707\" alt=\"스크린샷 2023-07-21 오후 1 22 20\" src=\"https://github.com/DolmaengC/DolmaengC.github.io/assets/107832431/7b1d89bf-9dab-447f-bba4-061d39e93eae\">\n<ul>\n<li>단계4 : 가중치를 Value 벡터에 곱한다.</li>\n</ul>\n<img width=\"703\" alt=\"스크린샷 2023-07-21 오후 1 22 40\" src=\"https://github.com/DolmaengC/DolmaengC.github.io/assets/107832431/988f1dbc-a832-4e2f-8aa9-1de8a355ad9b\">\n<ul>\n<li>최종 결과물\n<ul>\n<li>가중치가 곱해진 value vector들의 합</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<img width=\"805\" alt=\"스크린샷 2023-07-21 오후 1 22 57\" src=\"https://github.com/DolmaengC/DolmaengC.github.io/assets/107832431/1992c9bb-8aff-47dc-b597-c879559570ac\">\n<h3 id=\"transformer-1\" style=\"position:relative;\"><a href=\"#transformer-1\" aria-label=\"transformer 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Transformer</h3>\n<img width=\"826\" alt=\"스크린샷 2023-07-21 오후 5 16 41\" src=\"https://github.com/DolmaengC/DolmaengC.github.io/assets/107832431/98440e40-cada-4244-ad09-91514a5346c1\">\n<img width=\"776\" alt=\"스크린샷 2023-07-21 오후 5 21 59\" src=\"https://github.com/DolmaengC/DolmaengC.github.io/assets/107832431/96e15b3f-be72-42b5-9b33-7598da712a99\">\n<p>Masked Multi-head attention (핑크색 블럭): 인코더에서는 사용되지 않음</p>\n<hr>\n<img width=\"690\" alt=\"스크린샷 2023-07-21 오후 5 35 08\" src=\"https://github.com/DolmaengC/DolmaengC.github.io/assets/107832431/d3109c9d-13ee-4a3d-8bf5-3f05b550a605\">\n<ul>\n<li>Position-wise feed-forward network\n<ul>\n<li>2개 이상의 fully connected layer (혹은 dense layer)로 구성</li>\n<li>Token마다 (즉, position 마다) 독립적으로 적용</li>\n<li>첫번째 layer에 ReLU 활성화 함수 사용</li>\n</ul>\n</li>\n</ul>\n<hr>\n<img width=\"789\" alt=\"스크린샷 2023-07-21 오후 5 35 27\" src=\"https://github.com/DolmaengC/DolmaengC.github.io/assets/107832431/c1ec143e-9207-4838-974f-491be8326eaf\">\n<hr>\n<h4 id=\"위치정보-임베딩-positional-embedding\" style=\"position:relative;\"><a href=\"#%EC%9C%84%EC%B9%98%EC%A0%95%EB%B3%B4-%EC%9E%84%EB%B2%A0%EB%94%A9-positional-embedding\" aria-label=\"위치정보 임베딩 positional embedding permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>위치정보 임베딩 (Positional embedding)</h4>\n<ul>\n<li>Transformer 모형에서는 단어들의 embedding 정보만을 사용하는 것이 아니라 (단어들이 갖는) 입력된 시퀀스 데이터 내에서의 위치 정보도 사용</li>\n<li>위치 정보를 사용하게 되면, 단어들 간의 위치를 파악함으로써 단어들 간의 상대적인 거리를 파악</li>\n<li>이를 사용하는 주된 이유는, Transformer는 RNN이나 LSTM와 같은 순환신경만 구조(단어들이 순서대로 입력)를 사용하지 않고 attention 방법을 사용하기 때문</li>\n<li>단어들이 갖는 (상대적인) 위치정보를 반영하기 위해서 위치 정보를 반영하는 벡터를 사용하는데 이를 positionla embedding 벡터</li>\n</ul>\n<h4 id=\"masked-self-attention\" style=\"position:relative;\"><a href=\"#masked-self-attention\" aria-label=\"masked self attention permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Masked self-attention</h4>\n<ul>\n<li>Encoder의 self-attention과 약간 다르게 작동 -> 이해하기 위해서는 학습의 단계에서 transformer의 decoder 부분이 어떻게 작동하는지를 먼저 알아야 함</li>\n<li>Teacher forcing 방법 사용\n<ul>\n<li>decoder는 언어모형의 역할을 하지만, 학습의 단계에서는 모형이 현재 단계까지 예측한 단어들의 정보를 사용하여 다음 단어를 예측하는 것이 아니라, 정답 데이터 정보를 이용해서 각 단계의 단어들을 예측하는 것</li>\n<li>모형이 예측한 단어들의 정보를 이용해서 다음 단어를 예측하는 경우에는 이전 단어들에 대한 예측이 잘못되었다면 그 다음 단어에 대한 예측이 제대로 될수 없기 때문</li>\n<li>예 : ‘오늘은 금요일 입니다’을 ‘Today is Friday’로 번역하는 경우\n<ul>\n<li>첫 단어를 ‘Today’라고 예측하지 못하면, 그 다음 단어인 ‘is’를 제대로 예측할 수 없다</li>\n<li>이를 위해 학습에서는 실제 정답 데이터를 사용하여 학습한다.</li>\n<li>앞에 <SOS> 토큰을 더해서 사용 => 논문에서는 right shifted output이라고 표현</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"transformer-응용\" style=\"position:relative;\"><a href=\"#transformer-%EC%9D%91%EC%9A%A9\" aria-label=\"transformer 응용 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Transformer 응용</h4>\n<ul>\n<li>대표적 3가지\n<ul>\n<li>BERT (Bidirectional Encoder Representations from Transformers)\n<ul>\n<li>Transformer의 encoder 부분만을 사용</li>\n<li>주요 사용 용도\n<ul>\n<li>문서/ 단어 임베딩, 문서 분류, Q&#x26;A, 단어들 간의 관계 추출</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>GPT (Generative Pre-trained Transformer)\n<ul>\n<li>Transformer의 decoder 부분만을 사용</li>\n<li>주요 사용 용도\n<ul>\n<li>텍스트 생성</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>BART (Bidirectional and Auto-Regressive Transformers)\n<ul>\n<li>Transformer의 encoder와 decoder 모두 사용</li>\n<li>주요 사용 용도\n<ul>\n<li>텍스트 요약 등</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"bert\" style=\"position:relative;\"><a href=\"#bert\" aria-label=\"bert permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>BERT</h2>\n<h3 id=\"bert의-주-목적\" style=\"position:relative;\"><a href=\"#bert%EC%9D%98-%EC%A3%BC-%EB%AA%A9%EC%A0%81\" aria-label=\"bert의 주 목적 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>BERT의 주 목적</h3>\n<ul>\n<li>Text 분석에서의 전이 학습 (transfer learning)</li>\n<li>학습 데이터\n<ul>\n<li>BooksCorpus (800M words), English Wikipedia (2,500M words)</li>\n</ul>\n</li>\n<li>eㅏㄴ어와 문장의 임베딩 정보를 포함한 모형의 파라미터를 학습 -> downstream tast에 따라서 fine tuning 혹은 feature-based 방법으로 사용 가능</li>\n</ul>\n<h3 id=\"bert의-구조\" style=\"position:relative;\"><a href=\"#bert%EC%9D%98-%EA%B5%AC%EC%A1%B0\" aria-label=\"bert의 구조 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>BERT의 구조</h3>\n<ul>\n<li>Transfomer의 encoder 부분만 사용</li>\n<li>2가지 형태\n<ul>\n<li>BERTBASE (L=12, H=768, A=12, Total Parameters=110M)</li>\n<li>BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)</li>\n<li>L = encoder block의 수, H = 임베딩 벡터 또는 Hidden state 벡터의 차원의 수, A = multi-head attention에서 사용된 attentions의 수</li>\n</ul>\n</li>\n</ul>\n<p>![스크린샷 2023-07-21 오후 10.06.19](/Users/choejunhyeog/Desktop/스크린샷 2023-07-21 오후 10.06.19.png)</p>\n<ul>\n<li>새로운 토큰\n<ul>\n<li>\n<p>[CLS] : 입력 시퀀스 데이터 전체의 정보를 반영하기 위한 토큰</p>\n<ul>\n<li>이 토큰에 대한 hidden state는 보통 분류 등의 목적에 사용</li>\n</ul>\n</li>\n<li>\n<p>[SEP] : 두개의 문장을 구분하기 위한 목적</p>\n</li>\n</ul>\n</li>\n</ul>","excerpt":"Summary about Deep learning lecture Originally from Youtube lecture Activation function 주요 활성화 함수 logistic 함수 (Sigmoid function) : 요즘은 잘 안씀 Hyperblolic Tangent (tanh) : 텍스트, sequence data, 다른 분야에선 ㄴㄴ Rectified Linear Unit (Relu) : 이미지, 주로 많이 사용됨 Leaky Relu Exponential linear unit(ELU) 경사손실 문제 : 은닉층의 한 활성함수값이 0에 가까워지면 다른 은닉층의 값에 상관없이 0에 가까워지는 문제 -> 은닉층이 많을 수록 발생할 확률이 커져서 많이 쌓기 어렵다 -> Relu가 많이 사용되는 이유 Optimizer 주로 Adam, RMSprop, Adadelta가 사용됨 Momentum : 이전 업데이트 정보를 기억, 현재 업데이트에 반영하는 방법 장점 (GD에 비해…","frontmatter":{"date":"July 22, 2023","title":"Deep learning","categories":"DL","author":"DolmaengC","emoji":"🧢"},"fields":{"slug":"/Deep-learning/"}},"next":{"id":"895f4c31-0322-5f3a-aa40-b16741b44341","html":"<h3 id=\"cites\" style=\"position:relative;\"><a href=\"#cites\" aria-label=\"cites permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>CITES</h3>\n<p>Chen, L., Ouyang, Y., &#x26; Zhang, L. (2021, May). Fast and precise on-the-fly patch validation for all. In <em>2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)</em> (pp. 1123-1134). IEEE.</p>\n<h3 id=\"keywords\" style=\"position:relative;\"><a href=\"#keywords\" aria-label=\"keywords permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>KEYWORDS</h3>\n<p>Program repair, Program transformation, Runtime optimization, JVM bytecode manipulation</p>\n<h3 id=\"abstract\" style=\"position:relative;\"><a href=\"#abstract\" aria-label=\"abstract permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ABSTRACT</h3>\n<p>Generate-and-validate (G&#x26;V) automated program repair (APR) 기술은 지난 10년간 널리 연구되었다.</p>\n<p>한편, 그러한 기술은 많은 수의 패치를 만들고 잠재적 수정을 확인하기 위한 패치 테스트하기 위한 실행들 때문에 많은 시간이 걸린다.</p>\n<p>최근 G&#x26;V APR 기술인 PraPR은 프로그램 코드를 컴파일된 바이트코드 레벨로 바꿔서 코스트를 줄이고, 동일한 JVM 세션 내에서 여러 패치를 테스트할 수 있도록 하여 즉시 패칭을 추가로 수행하게 한다.</p>\n<p>하지만, PraPR은 패턴 기반, 바이트코드 레벨의 특성으로 인해 제한적이며 패치 실행이 글로벌 JVM 상태를 변경하지 않고, 동일한 JVM 세션에서 이후 패치 실행에 영향을 미친다고 가정하므로 기본적으로 불건전/부정확하다.</p>\n<p>PraPR 작업에서 영감을 받아 UniAPR이라는 통합 패치 유효성 검사 프레임워크를 제안한다. 즉석 패칭을 통해 바이트코드 및 소스 코드 APR 모두에 대한 패치 유효성 검사 속도를 높이는 것을 목표로 한다. 또한 UniAPR은 런타임 바이트코드 변환을 통해 JVM 전역 상태를 재설정하여 부정확한 패치 유효성 검사 문제를 해결한다.</p>\n<p>우리는 완전히 자동화된 메이븐 플러그인으로 UniAPR을 구현했다. 우리는 또한 최신 소스 코드 레벨 APR을 위한 즉성 패치 확인의 첫 번째 연구이다.</p>\n<p>우리의 실험은 바닐라 즉성 패치 유효성 검사가 부정확/불건전할 수 있다는 최초의 경험적 증거를 보여준다.</p>\n<p>대조적으로, 우리의 UniAPR 프레임워크는 패치 유효성 검사에서 부정확성을 초래하지 않고 최신 APR의 속도를 10배 이상 높일 수 있으므로 가까운 미래에 기존의 모든 APR 기술이 더 큰 검색 공간을 탐색하여 더 많은 버그를 수정할 수 있다.</p>\n<p>게다가, UniAPR은 하이브리드 소스 및 바이트코드 APR이 가까운 미래에 모든 최신 APR 기술(동일한 시간 제한 하에서)보다  훨씬 더 많은 버그를 수정할 수 있도록 직접 지원한다.</p>\n<h3 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>INTRODUCTION</h3>\n<p>테스트 기반 G&#x26;V APR 기술과 관련된 두 가지 주요 비용은 다음과 같습니다.</p>\n<p>(1) 특정 변환 규칙에 따라 패치를 제조/생성하기 위해 프로그램 코드를 조작하는 비용</p>\n<p>(2) 수정 중인 버그에 대한 그럴듯한 패치를 식별하기 위해 모든 개발자 테스트를 반복적으로 실행합니다.</p>\n<p>APR에 대한 검색 공간이 무한하고 이론적 한계로 인해 이 검색 공간의 요소를 분류하는 것이 불가능하기 때문에 테스트 기반 G&#x26;V APR 기술은 일반적으로 명확한 지침이 부족하고 거의 무차별 대입 방식으로 작동합니다. 유효성을 검사할 패치 풀과 프로그램이 클수록 생성하고 유효성을 검사할 패치 세트도 커집니다. 이는 패치 생성 및 검증 속도가 APR 기법의 확장성에 중요한 역할을 함을 시사하며, 이는 실제 APR 기법을 설계하는 데 있어 가장 중요한 과제 중 하나이다[9]. 따라서 새롭고 보다 효과적인 변환 규칙을 도입하는 것 외에도 앞서 언급한 비용을 완화하기 위해 일부 APR 기술이 제안되었습니다. 예를 들어 JAID[6]는 변형 스키마를 사용하여 단일 소스 파일에 여러 패치를 묶는 메타 프로그램을 제작하는 반면 SketchFix[17]는 스케치[28]를 사용하여 유사한 효과를 얻습니다. 그러나 이러한 기술은 주로 패치 생성 시간을 단축하는 것을 목표로 하는 반면 패치 유효성 검사 시간은 APR 동안 지배적인 것으로 나타났습니다[42]. 가장 최근에 PraPR[14]은 패치 생성 및 유효성 검사 시간을 모두 줄이는 것을 목표로 합니다. 컴파일된 JVM 바이트코드 수준에서 프로그램 코드를 직접 수정하여 패치 생성 비용을 줄이고 비용이 많이 드는 프로세스인 패치 간에 JVM(Java Virtual Machine) 세션 재사용을 통한 생성/초기화를 피함으로써 패치 유효성 검사 비용을 줄입니다.</p>\n<p>PraPR이 효과적이긴 하지만 2가지 주요한 문제가 있다.</p>\n<ol>\n<li>바이트코드의 성질로 인해 유연하지 않다.</li>\n<li>부정확한 패치 확인 결과로 인해 안전하지 않다.</li>\n</ol>\n<p>UniAPR은 모든 바이트코드 혹은 소스코드 레벨의 APR기술들을 위한 불필요한 JVM 재시작을 피함으로써 패치 확인 비용을 줄이는 결합된 테스트 기반 패치 확인 프레임워크이다.</p>\n<h3 id=\"approach\" style=\"position:relative;\"><a href=\"#approach\" aria-label=\"approach permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>APPROACH</h3>\n<h5 id=\"overview\" style=\"position:relative;\"><a href=\"#overview\" aria-label=\"overview permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Overview</h5>\n<img width=\"875\" alt=\"스크린샷 2023-01-12 오전 10 14 02\" src=\"https://user-images.githubusercontent.com/107832431/212212894-15147d7b-d1b0-43fa-9304-05dace8bae25.png\">\n<ol>\n<li>\n<p>UniAPR은 먼저 기존 APR 도구를 활용하여 소스코드 레벨 패치를 만든다(❶).</p>\n</li>\n<li>\n<p>UniAPR이 증분 컴파일을 수행하여 각 패치별로 패치된 소스 파일을 바이트코드 파일로 컴파일한다(❷).</p>\n<ul>\n<li>\n<p>UniAPR은 융합된 프레임워크이다</p>\n</li>\n<li>\n<p>UniAPR은 PraPR같은 바이트코드 APR기술을 통해 직접적으로 바이트코드를 얻을 수도 있다.</p>\n</li>\n<li>\n<p>이 방법으로 UniAPR은 패치 확인을 위한 바이트코드 패치 풀을 가진다.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>실제 패치 확인동안, UniAPR은 먼저 전체 버기 프로젝트를 바이트코드 파일로 컴파일한다(❸).</p>\n</li>\n<li>\n<p>그런 다음 모든 바이트코드 파일을 JVM 클래스 로더를 통해 JVM으로 로드한다(❹).</p>\n<ul>\n<li>이 두 단계는 버기 프로젝트에 대한 원래 테스트를 실행하는 것과 정확히 동일하다.</li>\n</ul>\n</li>\n<li>\n<p>원래 프로젝트의 모든 바이트코드 파일이 JVM 내에 로드되기 때문에 각 패치의 유효성을 검사하기 위해 UniAPR은 ❺로 표시된 Java Agent 기술 및 HotSpot 메커니즘을 통해 특정 패치에 의해 패치된 바이트코드 파일만 다시 로드한다.</p>\n</li>\n<li>\n<p>그 후, 테스트 드라이버를 실행하여 새로운 JVM을 재시작하지 않고 패치 확일을 위한 테스트를 실행할 수 있다.</p>\n</li>\n<li>\n<p>이 패치 실행에 대한 모든 테스트가 완료된 후 UniAPR은 패치된 바이트코드 파일을 원래 버전으로 되돌리기 위해 원래 파일로 교체한다.</p>\n</li>\n<li>\n<p>또한 UniAPR은 전역 JVM 상태를 재설정하여 다음 패치 실행을 위해 깨끗한 JVM 환경을 준비한다.(짧은 점선)</p>\n<ul>\n<li>모든 패치에 동일한 과정이 반복된다.</li>\n</ul>\n</li>\n<li>\n<p>마지막으로 패치 실행 결과는 소캣 연결을 통해 패치 실행 데이터베이스에 저장된다(❻).</p>\n<ul>\n<li>모든 테스트를 통과한 패치를 plausible 패치라 한다.</li>\n</ul>\n</li>\n<li>\n<p>소스코드 레벨 APR로 만들어진 패치들 중 plausible 패치인 경우 수동 검사를 위해 원본 소스코드 레벨 패치를 직접 검색한다.</p>\n</li>\n</ol>\n<h5 id=\"fast-patch-valudation-via-on-the-fly-patching\" style=\"position:relative;\"><a href=\"#fast-patch-valudation-via-on-the-fly-patching\" aria-label=\"fast patch valudation via on the fly patching permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Fast Patch Valudation via On-the-fly Patching</h5>\n<img width=\"527\" alt=\"스크린샷 2023-01-13 오후 3 01 09\" src=\"https://user-images.githubusercontent.com/107832431/212248622-d88f8dd9-06aa-483b-a3a4-d6ee4869d704.png\">","frontmatter":{"date":"July 22, 2023","title":"Fast and Precise On-the-fly Patch Validation for All","categories":"APR bytecode","author":"DolmaengC","emoji":"🧢"},"fields":{"slug":"/Uniapr/"}},"prev":{"id":"d9d7c0e9-58b3-51ff-84f1-8c6400dd81cf","html":"","frontmatter":{"date":"July 22, 2023","title":"창세기","categories":"Bible","author":"DolmaengC","emoji":"🧢"},"fields":{"slug":"/창세기/"}},"site":{"siteMetadata":{"siteUrl":"https://www.zoomkoding.com","comments":{"utterances":{"repo":""}}}}},"pageContext":{"slug":"/Deep-learning/","nextSlug":"/Uniapr/","prevSlug":"/창세기/"}},"staticQueryHashes":["1073350324","1956554647","2938748437"]}